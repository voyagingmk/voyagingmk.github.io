<!DOCTYPE html>
<html>
  <head>
    <title>线性代数之主成分分析(PCA)算法 – Wyman的技术博客 – 博主主要学习方向：图形学、机器学习，以及各种有趣的数学。联系QQ：234707482。</title>

        <meta charset="utf-8" />
    <meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>
    <meta name="baidu-site-verification" content="0OpfO1OtHA" />
    
    <meta name="description" content="PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。

另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。
" />
    <meta property="og:description" content="PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。

另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。
" />
    
    <meta name="author" content="Wyman的技术博客" />

    
    <meta property="og:title" content="线性代数之主成分分析(PCA)算法" />
    <meta property="twitter:title" content="线性代数之主成分分析(PCA)算法" />
    

    <!--[if lt IE 9]>
      <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <link rel="stylesheet" type="text/css" href="/style.css" />
    <link rel="alternate" type="application/rss+xml" title="Wyman的技术博客 - 博主主要学习方向：图形学、机器学习，以及各种有趣的数学。联系QQ：234707482。" href="/feed.xml" />

    
	<!-- Google Analytics -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-65954265-1', 'auto');
		ga('send', 'pageview', {
		  'page': '/linear-algebra-17/',
		  'title': '线性代数之主成分分析(PCA)算法'
		});
	</script>
	<!-- End Google Analytics -->
	<!-- Baidu Analytics -->
	<script>
		var _hmt = _hmt || [];
		(function() {
		  var hm = document.createElement("script");
		  hm.src = "//hm.baidu.com/hm.js?0dc968591d8c64196a37eca9ca4f86b3";
		  var s = document.getElementsByTagName("script")[0]; 
		  s.parentNode.insertBefore(hm, s);
		})();
	</script>
	<!-- End Baidu Analytics -->

    <!-- Created with Jekyll Now - http://github.com/barryclark/jekyll-now -->
  </head>

  <body>
    <div class="wrapper-masthead">
      <div class="container">
        <header class="masthead clearfix">
          <a href="/" class="site-avatar"><img src="http://www.qiujiawei.com/images/avatar.jpg" /></a>

          <div class="site-info">
            <h1 class="site-name"><a href="/">Wyman的技术博客</a></h1>
            <p class="site-description">博主主要学习方向：图形学、机器学习，以及各种有趣的数学。联系QQ：234707482。</p>
          </div>

          <nav>
            <a href="/">Blog</a>
            <a href="/about">About</a>
          </nav>
        </header>
      </div>
    </div>

    <div id="main" role="main" class="container">
      <script type="text/javascript"
            src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<article class="post">
  <h1>线性代数之主成分分析(PCA)算法</h1>
  <h3>Tags: <a href="/tag/matrix.html" rel="tag">matrix</a>, <a href="/tag/math.html" rel="tag">math</a></h3>
  <div class="entry">
    <p>PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。</p>

<p>另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。</p>

<!--more-->

<h1>Part I</h1>

<p>设向量\( \vec x \)的每个分量分别记录了某一个特征的信息，且共有n个分量(特征)；那么m个不同的\( \vec x \)就组成了一个\(m\times n \)的矩阵\( X \)：</p>

<p>\[ X =  \left[ \begin{matrix} \vec x_{1}\\   \vec x_{2}\\   \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>然后问题来了：\( \vec x \)的每个分量之间是否是<strong>相互独立(independant)</strong>的？如果是，那么说明这n个特征是良好的，可以直接拿去应用到任务中(譬如基于这些特征做一个分类器)；如果不是，那么就说明有特征是多余的，譬如\( x^{(k)} \)、\( x^{(k+1)} \)分别用米和英尺记录了同一个特征，虽然数值不一样，然而并没有什么卵用。</p>

<p>量化特征与特征之间的关系的最好办法是用<strong>方差</strong>(<a href="https://en.wikipedia.org/wiki/Variance">Variance</a>)和<strong>协方差</strong>(<a href="https://en.wikipedia.org/wiki/Covariance">Covariance</a>)，这2者又共同涉及到了更基础的概念<strong>数学期望</strong>(<a href="https://en.wikipedia.org/wiki/Expected_value">Expected Value</a>)和<strong>均值</strong>(<a href="https://en.wikipedia.org/wiki/Mean">Mean</a>)。先简单过一遍这4个东西的公式。</p>

<h3>数学期望和均值</h3>

<p>\[ E[\vec x] = \sum _{i=1}^{n}x_{i}p_{i} \]</p>

<p>当每个\( x_{i} \)的出现概率相等时(均匀分布)，有\( p_{i} = \frac {1}{n} \)，所以上式可简化成:</p>

<p>\[ E[\vec x] = \frac {1}{n}\sum _{i=1}^{n }x_{i} \]</p>

<p>上式其实也就是均值\( \overline {x} \)的定义，所以当\(x_{i}\)均匀分布时，有：</p>

<p>\[  E[\vec x] =  \overline {x} \]</p>

<p>有时候也用\( \mu \)来指代Mean。</p>

<h3>方差和协方差</h3>

<p>方差:</p>

<p>\[ Var(\vec x) = E[ (\vec x - E[\vec x])^{2 } ] = E[ (\vec x - E[\vec x])(\vec x -  E[\vec x]) ]  \]</p>

<p>协方差:</p>

<p>\[ Cov(\vec x, \vec y) = E[ (\vec x -  E[\vec x])(\vec y -  E[\vec y]) ] \]</p>

<p>可以发现方差是协方差的特殊情况:</p>

<p>\[ Var(\vec x) = Cov(\vec x, \vec x) \]</p>

<h3>协方差矩阵</h3>

<p>在<a href="http://daobiao.win:4000/linear-algebra-7/">线性代数之各种各样的矩阵</a>最后面已经提到了协方差矩阵(Covariance matrix):</p>

<p>\[ C =  \left[ \begin{matrix} E[(\vec x_{1} - \mu_{1})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{1} - \mu_{1})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{1} - \mu_{1})(\vec x_{m} - \mu_{m})]\\            E[(\vec x_{2} - \mu_{2})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{2} - \mu_{2})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{2} - \mu_{2})(\vec x_{m} - \mu_{m})]\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\         E[(\vec x_{m} - \mu_{m})(\vec x_{1} - \mu_{1})]&amp;  E[(\vec x_{m} - \mu_{m})(\vec x_{2} - \mu_{2})]&amp;  \cdots &amp; E[(\vec x_{m} - \mu_{m})(\vec x_{m} - \mu_{m})]\\ \end{matrix} \right]  \]</p>

<h3>当Mean等于0时的情况</h3>

<p>当Mean等于0时，上面的协方差矩阵变成：</p>

<p>\[ C =  \left[ \begin{matrix} E[\vec x_{1}\vec x_{1}]&amp;  E[\vec x_{1} \vec x_{2}]&amp;  \cdots &amp; E[\vec x_{1}\vec x_{m}]\\            E[\vec x_{2}\vec x_{1}]&amp;  E[\vec x_{2}\vec x_{2}]&amp;  \cdots &amp; E[\vec x_{2}\vec x_{m}]\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\         E[\vec x_{m}\vec x_{1}]&amp;  E[\vec x_{m}\vec x_{2}]&amp;  \cdots &amp; E[\vec x_{m}\vec x_{m}]\\ \end{matrix} \right]  \]</p>

<p>再假设\( \vec x \)每个分量的取值是均匀分布的，那么根据上面的定义，有：</p>

<p>\[E[\vec x_{a}\vec x_{b}] = \frac {1}{n}\sum _{i=1}^{n} \vec x_{ai}\vec x_{bi} , 1 \leq a\leq m, 1 \leq b\leq m  \]</p>

<p>代入上式，得到：</p>

<p>\[ C = \frac {1}{n} \left[ \begin{matrix} \sum _{i=1}^{n} \vec x_{1}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{1}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{1}\vec x_{m}\\            \sum _{i=1}^{n} \vec x_{2}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{2}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{2}\vec x_{m}\\   \vdots &amp; \vdots &amp;  \ddots &amp; \vdots \\        \sum _{i=1}^{n} \vec x_{m}\vec x_{1}&amp;  \sum _{i=1}^{n} \vec x_{m}\vec x_{2}&amp;  \cdots &amp; \sum _{i=1}^{n} \vec x_{m}\vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>再设一个矩阵X：</p>

<p>\[ X =  \left[ \begin{matrix} \vec x_{1}\\  \vec x_{2}\\  \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp; \vec x_{2}&amp; \cdots &amp; \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>于是有：</p>

<p>\[ C = \frac {1}{n}XX^{T} \]</p>

<p><strong>总结下</strong>，对符合均匀分布的、且均值等于0的\(\vec x_{i, 1\leq i \leq m}\)，它的协方差矩阵如下：</p>

<p>\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp; \vec x_{2}&amp; \cdots &amp; \vec x_{m}\\ \end{matrix} \right]  \]</p>

<p>\[ C = \frac {1}{n}XX^{T} \]</p>

<p>嘿嘿嘿</p>

  </div>

  <div class="date">
    Written on May 30, 2016
  </div>

  
<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">

	    var disqus_shortname = 'qiujiawei';

	    (function() {
	        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
	        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
	        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	    })();

	</script>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

</article>


    </div>

    <div class="wrapper-footer">
      <div class="container">
        <footer class="footer">
          
<a href="mailto:voyagingmk@gmail.com"><i class="svg-icon email"></i></a>


<a href="http://github.com/barryclark/jekyll-now"><i class="svg-icon github"></i></a>




<a href="http://twitter.com/voyagingmk"><i class="svg-icon twitter"></i></a>


        </footer>
      </div>
    </div>

  </body>
</html>
