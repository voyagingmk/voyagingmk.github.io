<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wyman的技术博客</title>
    <description>博主主要学习方向：图形学、机器学习，以及各种有趣的数学。联系QQ：234707482。</description>
    <link>http://www.qiujiawei.com</link>
    <atom:link href="http://www.qiujiawei.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>线性代数之主成分分析(PCA)算法</title>
        <description>&lt;p&gt;PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。&lt;/p&gt;

&lt;p&gt;另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1&gt;Part I&lt;/h1&gt;

&lt;p&gt;设向量\( \vec x \)的每个分量分别记录了某一个特征的信息，且共有n个分量(特征)；那么m个不同的\( \vec x \)就组成了一个\(m\times n \)的矩阵\( X \)：&lt;/p&gt;

&lt;p&gt;\[ X =  \left[ \begin{matrix} \vec x_{1}\\   \vec x_{2}\\   \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;然后问题来了：\( \vec x \)的每个分量之间是否是&lt;strong&gt;相互独立(independant)&lt;/strong&gt;的？如果是，那么说明这n个特征是良好的，可以直接拿去应用到任务中(譬如基于这些特征做一个分类器)；如果不是，那么就说明有特征是多余的，譬如\( x^{(k)} \)、\( x^{(k+1)} \)分别用米和英尺记录了同一个特征，虽然数值不一样，然而并没有什么卵用。&lt;/p&gt;

&lt;p&gt;量化特征与特征之间的关系的最好办法是用&lt;strong&gt;方差&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;)和&lt;strong&gt;协方差&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance&quot;&gt;Covariance&lt;/a&gt;)，这2者又共同涉及到了更基础的概念&lt;strong&gt;数学期望&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot;&gt;Expected Value&lt;/a&gt;)和&lt;strong&gt;均值&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean&quot;&gt;Mean&lt;/a&gt;)。先简单过一遍这4个东西的公式。&lt;/p&gt;

&lt;h3&gt;数学期望和均值&lt;/h3&gt;

&lt;p&gt;\[ E[\vec x] = \sum _{i=1}^{n}x_{i}p_{i} \]&lt;/p&gt;

&lt;p&gt;当每个\( x_{i} \)的出现概率相等时(均匀分布)，有\( p_{i} = \frac {1}{n} \)，所以上式可简化成:&lt;/p&gt;

&lt;p&gt;\[ E[\vec x] = \frac {1}{n}\sum _{i=1}^{n }x_{i} \]&lt;/p&gt;

&lt;p&gt;上式其实也就是均值\( \overline {x} \)的定义，所以当\(x_{i}\)均匀分布时，有：&lt;/p&gt;

&lt;p&gt;\[  E[\vec x] =  \overline {x} \]&lt;/p&gt;

&lt;p&gt;有时候也用\( \mu \)来指代Mean。&lt;/p&gt;

&lt;h3&gt;方差和协方差&lt;/h3&gt;

&lt;p&gt;方差:&lt;/p&gt;

&lt;p&gt;\[ Var(\vec x) = E[ (\vec x - E[\vec x])^{2 } ] = E[ (\vec x - E[\vec x])(\vec x -  E[\vec x]) ]  \]&lt;/p&gt;

&lt;p&gt;协方差:&lt;/p&gt;

&lt;p&gt;\[ Cov(\vec x, \vec y) = E[ (\vec x -  E[\vec x])(\vec y -  E[\vec y]) ] \]&lt;/p&gt;

&lt;p&gt;可以发现方差是协方差的特殊情况:&lt;/p&gt;

&lt;p&gt;\[ Var(\vec x) = Cov(\vec x, \vec x) \]&lt;/p&gt;

&lt;h3&gt;协方差矩阵&lt;/h3&gt;

&lt;p&gt;在&lt;a href=&quot;http://daobiao.win:4000/linear-algebra-7/&quot;&gt;线性代数之各种各样的矩阵&lt;/a&gt;最后面已经提到了协方差矩阵(Covariance matrix):&lt;/p&gt;

&lt;p&gt;\[ C =  \left[ \begin{matrix} E[(\vec x_{1} - \mu_{1})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{1} - \mu_{1})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{1} - \mu_{1})(\vec x_{m} - \mu_{m})]\\            E[(\vec x_{2} - \mu_{2})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{2} - \mu_{2})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{2} - \mu_{2})(\vec x_{m} - \mu_{m})]\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\         E[(\vec x_{m} - \mu_{m})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{m} - \mu_{m})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{m} - \mu_{m})(\vec x_{m} - \mu_{m})]\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;h3&gt;当Mean等于0时的情况&lt;/h3&gt;

&lt;p&gt;当Mean等于0时，上面的协方差矩阵变成：&lt;/p&gt;

&lt;p&gt;\[ C =  \left[ \begin{matrix} E[\vec x_{1}\vec x_{1}]&amp;amp;  E[\vec x_{1} \vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{1}\vec x_{m}]\\            E[\vec x_{2}\vec x_{1}]&amp;amp;  E[\vec x_{2}\vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{2}\vec x_{m}]\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\         E[\vec x_{m}\vec x_{1}]&amp;amp;  E[\vec x_{m}\vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{m}\vec x_{m}]\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;再假设\( \vec x \)每个分量的取值是均匀分布的，那么根据上面的定义，有：&lt;/p&gt;

&lt;p&gt;\[E[\vec x_{a}\vec x_{b}] = \frac {1}{n}\sum _{i=1}^{n} \vec x_{ai}\vec x_{bi} , 1 \leq a\leq m, 1 \leq b\leq m  \]&lt;/p&gt;

&lt;p&gt;代入上式，得到：&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n} \left[ \begin{matrix} \sum _{i=1}^{n} \vec x_{1}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{1}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{1}\vec x_{m}\\            \sum _{i=1}^{n} \vec x_{2}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{2}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{2}\vec x_{m}\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\        \sum _{i=1}^{n} \vec x_{m}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{m}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{m}\vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;再设一个矩阵X：&lt;/p&gt;

&lt;p&gt;\[ X =  \left[ \begin{matrix} \vec x_{1}\\  \vec x_{2}\\  \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp;amp; \vec x_{2}&amp;amp; \cdots &amp;amp; \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;于是有：&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n}XX^{T} \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结下&lt;/strong&gt;，对符合均匀分布的、且均值等于0的\(\vec x_{i, 1\leq i \leq n}\)，它的协方差矩阵如下：&lt;/p&gt;

&lt;p&gt;\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp;amp; \vec x_{2}&amp;amp; \cdots &amp;amp; \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n}XX^{T} \]&lt;/p&gt;
</description>
        <pubDate>Mon, 30 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-17/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-17/</guid>
      </item>
    
      <item>
        <title>线性代数之伪逆矩阵(pseudoinverse matrix)</title>
        <description>&lt;p&gt;众所周知只有方阵才有逆矩阵，非方阵没有逆矩阵。这个不和谐的问题已在20世纪初被数学家&lt;a href=&quot;https://en.wikipedia.org/wiki/E._H._Moore&quot;&gt;E. H. Moore&lt;/a&gt;等人解决掉了，因为他们发明了&lt;strong&gt;一般化的逆矩阵(generalized inverse)&lt;/strong&gt;，也称为&lt;strong&gt;伪逆矩阵(Moore–Penrose pseudoinverse)&lt;/strong&gt;。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1&gt;定义&lt;/h1&gt;

&lt;p&gt;对于任意一个矩阵A，A的伪逆矩阵\(A ^{+} \)必然存在，且\(A ^{+} \)必然满足以下四个条件：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;\( AA ^{+}A = A \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( A ^{+}AA ^{+} = A ^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (AA ^{+})^{*} = AA ^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A ^{+}A)^{*} = A ^{+}A \)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这四个条件(性质)蕴含了一个事情：\(AA ^{+} \)必然是一个效果等同单位矩阵I、但又不是单位矩阵I的矩阵。&lt;/p&gt;

&lt;p&gt;伪逆矩阵\( A ^{+} \)的极限形式定义：&lt;/p&gt;

&lt;p&gt;\[ A^{+} = \lim _{\delta \searrow 0} ( A^{*}A + \delta I ) ^{-1}A^{*} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{\delta \searrow 0}A^{*} ( A^{*}A + \delta I ) ^{-1} \]&lt;/p&gt;

&lt;p&gt;伪逆矩阵更加常用的定义（基于SVD奇异值分解）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SVD公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ A = UΣV^{*} \]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;伪逆矩阵公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ A^{+} = VΣ^{+}U^{*} \]&lt;/p&gt;

&lt;p&gt;这个公式要注意的是中间的\(Σ^{+}\)的求法。因为\(Σ_{m\times n}\)是一个对角线矩阵，但又不一定是方阵，所以计算它的伪逆矩阵的步骤是特殊又简单的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;将对角线上的元素取倒数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;再将整个矩阵转置一次&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;性质&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当A可逆时，A的伪逆矩阵等于A的逆矩阵&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;零矩阵的伪逆矩阵是它的转置矩阵&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{+})^{+} = A \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{+})^{T} = (A^{T})^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( ( \overline {A} )^{+} =  \overline { A^{+} } \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{*})^{+}  = (A^{+})^{*} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (\alpha A)^{+}  = \alpha ^{-1}A^{+} \)，\(\alpha \)不等于0&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;应用：最小二乘估计&lt;/h1&gt;

&lt;p&gt;在我的&lt;a href=&quot;http://www.qiujiawei.com/linear-algebra-15/&quot;&gt;最小二乘估计(Least Squares Estimator)的公式的推导&lt;/a&gt;一文中，提到了一个小问题，&lt;strong&gt;当矩阵X不是方阵时，最小二乘估计公式必须为&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;不能进一步简化，除非X是有逆矩阵的方阵。&lt;/p&gt;

&lt;p&gt;这个事情也可以用伪逆矩阵来讨论一遍。&lt;/p&gt;

&lt;p&gt;先回到问题本源——最小二乘估计的本质是解决下面的方程：&lt;/p&gt;

&lt;p&gt;\[ \vec y = X\vec a + \vec e \]&lt;/p&gt;

&lt;p&gt;其中\(\vec y\)和\(X\)是已知量，\(\vec a\)和\(\vec e\)是要求的量，这可能有0到n个解，而我们的目标是想找使得\( ||\vec e||_{2}\)最小的\(\vec a\)。&lt;/p&gt;

&lt;p&gt;当我们求得理想的\(\vec a\)、\(\vec e\)后，可以让\(\vec e = \vec 0\)，并把\(\vec a\)、\(\vec e\)代入原方程，从而得到下面的等式：&lt;/p&gt;

&lt;p&gt;\[ \widehat {\vec y} = X\vec a \]&lt;/p&gt;

&lt;p&gt;求得的\( \widehat {\vec y} \)就是\( \vec y  \)的最佳近似。&lt;/p&gt;

&lt;p&gt;再换个角度想——如果我们一开始就默认方程\( \vec y = X\vec a  \)有解，那么这个解就是：&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}\vec y \]&lt;/p&gt;

&lt;p&gt;慢着！\(  X^{-1} \)可不一定存在的，因为X不一定是方阵，所以上面这个等式是错误的。怎么办？这时候伪逆矩阵就派上用场了：&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{+}\vec y \]&lt;/p&gt;

&lt;p&gt;因为伪逆矩阵对任意矩阵都存在，所以这个等式才是合理的。&lt;/p&gt;

&lt;h1&gt;参考资料&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse#Singular_value_decomposition_.28SVD.29&quot;&gt;Moore–Penrose pseudoinverse&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 29 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-16/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-16/</guid>
      </item>
    
      <item>
        <title>矩阵微分（一）</title>
        <description>&lt;h1&gt;基本认识&lt;/h1&gt;

&lt;h2&gt;3种标准导数(梯度)公式&lt;/h2&gt;

&lt;p&gt;1) 自变量是一个数值量(Scalar)时：&lt;/p&gt;

&lt;p&gt;\[ Df(x) = \lim _{t\to 0} \frac {f(x+t)-f(t)}{t} \]&lt;/p&gt;

&lt;p&gt;2) 自变量是一个向量(Vector)时：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {w}}f(\textbf {x}) = \lim _{t\to 0} \frac {f(\textbf {x} + t\textbf {w}) - f(t)}{t} \]&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;(w的维数和x一致)&lt;/p&gt;

&lt;p&gt;这个导数的含义是，在n维空间中f(x)所定义的(超)平面上的某个坐标点x相对于w的斜率。&lt;/p&gt;

&lt;p&gt;3) 自变量是一个矩阵(Matrix)时：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;含义和2)类似。（已经无法想象了）&lt;/p&gt;

&lt;h2&gt;矩阵迹(trace)的各种性质&lt;/h2&gt;

&lt;h3&gt;性质1&lt;/h3&gt;

&lt;p&gt;\[ tr(A) = tr(A^{T}) \]&lt;/p&gt;

&lt;h3&gt;性质2&lt;/h3&gt;

&lt;p&gt;\[ tr(AB) = tr(BA) \]&lt;/p&gt;

&lt;p&gt;\[ tr(ABC) = tr(CAB) = tr(BCA) \]&lt;/p&gt;

&lt;p&gt;\[ tr(ABCD) = tr(DABC) = tr(CDAB) = tr(BCDA) \]&lt;/p&gt;

&lt;p&gt;(看出规律了吧)&lt;/p&gt;

&lt;h3&gt;性质3&lt;/h3&gt;

&lt;p&gt;\[ tr(A+B) = tr(A) + tr(B) \]&lt;/p&gt;

&lt;h3&gt;性质4&lt;/h3&gt;

&lt;p&gt;\[ tr(\alpha A) = \alpha tr(A) \]&lt;/p&gt;

&lt;h3&gt;性质5&lt;/h3&gt;

&lt;p&gt;设有矩阵H、U，H和U都是n x m的矩阵，则有：&lt;/p&gt;

&lt;p&gt;\[ \sum _{j=1}^{m} \sum _{i=1}^{n}(h_{ij}u_{ij}) = \sum _{j=1}^{m} \sum _{i=1}^{n}((h^{T})_{ji}u_{ij}) = tr(H^{T}U) \]&lt;/p&gt;

&lt;h1&gt;矩阵微分的各种性质&lt;/h1&gt;

&lt;p&gt;设有关于矩阵A的一个函数f，记为\( f(A) \)，\( f(A) \)关于A的导数为：&lt;/p&gt;

&lt;p&gt;\[  \nabla _{A}f(A) = \frac { \partial f(A) }{ \partial A } \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{12}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{1n}}\\ \frac {\partial f }{\partial A_{21}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{2n}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{m1}}&amp;amp;\frac {\partial f }{\partial A_{m2}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h2&gt;性质1&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ A^{T} }f(A) = (\nabla _{A}f(A))^{T} \]&lt;/p&gt;

&lt;p&gt;证明：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ A^{T} }f(A) = \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{21}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{m1}}\\ \frac {\partial f }{\partial A_{12}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{m2}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{1n}}&amp;amp;\frac {\partial f }{\partial A_{2n}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{12}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{1n}}\\ \frac {\partial f }{\partial A_{21}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{2n}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{m1}}&amp;amp;\frac {\partial f }{\partial A_{m2}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right]^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = (\frac { \partial f(A) }{ \partial A })^{T}  = (\nabla _{A}f(A))^{T} \]&lt;/p&gt;

&lt;h2&gt;性质2&lt;/h2&gt;

&lt;p&gt;假设存在矩阵U，使得下面的等式成立：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} = tr(W^{T}U) \]&lt;/p&gt;

&lt;p&gt;（这里要注意一下：中间的式子的分子包含矩阵，而分母只是一个t，那么这个极限的结果应仍然仍是一个矩阵，然而等式右边却是一个trace，trace是一个数。矩阵等于数？其实是可以的，譬如当f(X)是一个tr运算的时候。）&lt;/p&gt;

&lt;p&gt;那么，对\( \textbf {W} \)中任意一个\(W_{ij} \)求导，则有：&lt;/p&gt;

&lt;p&gt;\[ D_{W_{ij}}f(\textbf {X}) = tr(W_{ij}^{T}U) = \sum _{j=1}^{} \sum _{i=1}^{}(w_{ij}u_{ij}) = u_{ij}  \]&lt;/p&gt;

&lt;p&gt;这个结果可能有点费解。首先要明白\( W_{ij} \)是W矩阵的一个元素，是一个值(Scalar)，那么就可以确定这个导数也是一个值(Scalar)；对W矩阵的局部单个元素求导，其实按偏导数的概念理解即可，既然是偏导数，这就意味着除了存在\( w_{ij} \)的那一项之外的其他元素都被当做常数，而对常数求导必然等于0，所以最后会得到唯一的\( u_{ij}\)。(中间那一步用到了trace的性质5)&lt;/p&gt;

&lt;p&gt;既然已经知道，对局部的\( W_{ij} \)求导会得到\( U_{ij}\)，那么分别对所有\( W_{ij} \)求导，并把各个求导结果再组成一个矩阵，就是U矩阵了。又因为W代表任意矩阵，所以f(X)关于X的导数等于U：&lt;/p&gt;

&lt;p&gt;\[  \frac { \partial f(\textbf {X}) }{ \partial X } = \textbf {U} \]&lt;/p&gt;

&lt;p&gt;这个式子的意义在于，当题目是“给你一个自变量是矩阵X的函数f(X),求它关于X的导数”时，可以把问题立即转变成求U，而U的求解，可以通过上面的标准导数公式来求。小结一下步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;计算\( \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \)，并化简，直到得到一个形如\(tr(W^{T}Q) \)的式子；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据\( \frac { \partial f(\textbf {X}) }{ \partial X } = \textbf {U} \)，可以知道\(tr(W^{T}Q) \) = \(tr(W^{T}U) \)， 于是就得到了\(\frac { \partial f(\textbf {X}) }{ \partial X } = U = Q \) 。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;性质3&lt;/h2&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } = A^{T} \]&lt;/p&gt;

&lt;p&gt;证明过程需要用到上面的性质2，刚好作为一个应用举例。&lt;/p&gt;

&lt;p&gt;证明，设：&lt;/p&gt;

&lt;p&gt;\[ f(X) = tr(AX)   \]&lt;/p&gt;

&lt;p&gt;根据上面的结论，只需要把下面这个极限简化，理论上就可以求出 \( \frac { \partial tr(AX) }{ \partial X } \) 了：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(A(X + tW)) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AX + AtW) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AX) + tr(AtW) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AtW) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AW)t }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} tr(AW) \]&lt;/p&gt;

&lt;p&gt;\[  = tr(AW) \]&lt;/p&gt;

&lt;p&gt;\[  = tr((AW)^{T}) \]&lt;/p&gt;

&lt;p&gt;\[  = tr(W^{T}A^{T}) \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ D_{W}f(X) = tr(W^{T}A^{T}) = tr(W^{T}U)  \]&lt;/p&gt;

&lt;p&gt;\[ U = A^{T} \]&lt;/p&gt;

&lt;p&gt;得证：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } = U =  A^{T}  \] &lt;/p&gt;

&lt;h2&gt;性质4&lt;/h2&gt;

&lt;p&gt;\[ \frac { \partial tr(X^{T}A^{T}) }{ \partial X } = A^{T} \]&lt;/p&gt;

&lt;p&gt;有了性质3，就可以推导出这个：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr((AX)^{T}) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(X^{T}A^{T}) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;h2&gt;性质5&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(X) = tr(\nabla _{ X}X) \]&lt;/p&gt;

&lt;p&gt;待证。&lt;/p&gt;

&lt;h2&gt;性质6&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(AXBX^{T}C) = A^{T}C^{T}XB^{T} + CAXB \]&lt;/p&gt;

&lt;p&gt;类似性质3的证明过程，只是复杂一些。设:&lt;/p&gt;

&lt;p&gt;\[ f(X) = tr(AXBX^{T}C)  \]&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(A(X + tW)B(X + tW)^{T}C) -  tr(AXBX^{T}C) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( A(X + tW)B(X + tW)^{T}C - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( (AXB + tAWB)(X^{T}C + tW^{T}C) - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( AXBX^{T}C + tAWBX^{T}C + tAXBW^{T}C + t^{2}AWBW^{T}C - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( tAWBX^{T}C + tAXBW^{T}C + t^{2}AWBW^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( AWBX^{T}C + AXBW^{T}C + tAWBW^{T}C )t }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C + tAWBW^{T}C )] \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C)] + \lim _{t\to 0} [tr( tAWBW^{T}C )] \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C)] \]&lt;/p&gt;

&lt;p&gt;\[  = tr( AWBX^{T}C ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( (AWBX^{T}C)^{T} ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( C^{T}XB^{T}W^{T}A^{T} ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}A^{T}C^{T}XB^{T} ) + tr( W^{T}CAXB ) 【用了trace的性质2】 \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}A^{T}C^{T}XB^{T} + W^{T}CAXB ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}  (A^{T}C^{T}XB^{T} + CAXB) ) \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ D_{W}f(X) = tr( W^{T}  (A^{T}C^{T}XB^{T} + CAXB) ) = tr(W^{T}U)  \]&lt;/p&gt;

&lt;p&gt;得证：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AXBX^{T}C) }{ \partial X } = U = A^{T}C^{T}XB^{T} + CAXB \]&lt;/p&gt;

&lt;p&gt;在wiki&lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus&quot;&gt;Matrix Calculus&lt;/a&gt;还给出了用性质5公式：\( \nabla _{ X}tr(X) = tr(\nabla _{ X}X) \)推导性质6。为了方便，把图也贴进来吧：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/c/6/1/c61612bff41e8572a97d977871ce2be2.png&quot; alt=&quot;https://upload.wikimedia.org/math/c/6/1/c61612bff41e8572a97d977871ce2be2.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/1/e/7/1e761891d19891ff75670424341b8425.png&quot; alt=&quot;https://upload.wikimedia.org/math/1/e/7/1e761891d19891ff75670424341b8425.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;它最后得到的式子和我的推导似乎不一样，但其实是一样的，用trace的性质1\( tr(A) = tr(A^{T}) \)可以转换得到。&lt;/p&gt;

&lt;h2&gt;性质7&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(XBX^{T}C) = C^{T}XB^{T} + CXB \]&lt;/p&gt;

&lt;p&gt;证明：把性质6的A设为单位矩阵E，得到：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ X}tr(EXBX^{T}C) = E^{T}C^{T}XB^{T} + CEXB \]&lt;/p&gt;

&lt;p&gt;化简得到：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ X}tr(XBX^{T}C) = C^{T}XB^{T} + CXB \]&lt;/p&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.tc.umn.edu/%7Enydic001/docs/unpubs/Schonemann_Trace_Derivatives_Presentation.pdf&quot;&gt;http://www.tc.umn.edu/~nydic001/docs/unpubs/Schonemann_Trace_Derivatives_Presentation.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 08 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/matrix-calculus-1/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/matrix-calculus-1/</guid>
      </item>
    
      <item>
        <title>最小二乘估计(Least Squares Estimator)的公式的推导</title>
        <description>&lt;p&gt;最近在学习ML(Machine Learning)，注意到了一个有趣的东西：&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)&quot;&gt;Least Squares Estimator&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;先从简单说起吧。看下面的式子：&lt;/p&gt;

&lt;p&gt;\[ y = ax + e \]&lt;/p&gt;

&lt;p&gt;这是一个非常简单的直线方程。如果赋予y、a、x、b具体的意义，这个式子就有意思了：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;假设x是一个统计变量（预先就知道的），譬如，x代表人的年龄。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;假设y是关于x的一个label量（预先就知道的），譬如，y代表的是年龄为x时的人的智商。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;假设y和x存在线性关系，那么可以有 y = ax。这个式子表明年龄为x时，智商为ax。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当x、y的取值只有一对时，a = y/x，但当x、y不只一对时，y = ax可能会无解（因为求解的是方程组 \( y_{i} = ax_{i} \) 了）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了使方程组 \( y_{i} = ax_{i} \) 可以求解，需要把方程组扩展成 \( y_{i} = ax_{i} + e_{i} \) 。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( y_{i} = ax_{i} + e_{i} \)使得我们有机会求出a，但同时也产生了很多个\( e_{i} \)。每对&lt;y,  x&gt;都有它自己的error系数的话，这个a的意义就减弱了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了使得a变得更有意义，我们希望每个error系数尽可能地小（无限逼近0最好了），同时又能求出唯一的a。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;又因为现实生活中，智商肯定不只跟年龄x有关系，还和其他参数有关系，那么可以再把公式扩展成:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\[ y_{i} = a_{1}x_{i1} + a_{2}x_{i2} + \cdots + a_{k}x_{ik} + e_{i} ,   1\le i\le n,   k\ge 1 \]&lt;/p&gt;

&lt;p&gt;现在，把上式写成矩阵形式：&lt;/p&gt;

&lt;p&gt;\[ \vec y = X\vec a + \vec e \]&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} y_{1}\\   y_{2}\\   \vdots \\   y_{n}\\ \end{matrix} \right] =  \left[ \begin{matrix} x_{11}&amp;amp;  x_{12}&amp;amp;  \cdots &amp;amp;  x_{1k}\\      x_{21}&amp;amp;  x_{22}&amp;amp;  \cdots &amp;amp;  x_{2k}\\      \vdots &amp;amp;  \vdots &amp;amp;  \ddots &amp;amp;  \vdots \\     x_{n1}&amp;amp;  x_{n2}&amp;amp;  \cdots &amp;amp;  x_{nk}\\ \end{matrix} \right] \left[ \begin{matrix} a_{1}\\   a_{2}\\   \vdots \\   a_{k}\\ \end{matrix} \right] +  \left[ \begin{matrix} e_{1}\\   e_{2}\\   \vdots \\   e_{n}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;再回到上面的第7点：为了使得\(\vec a\)变得更有意义，我们希望\(\vec e\)的每个分量尽可能地小。明确这一点非常重要。&lt;/p&gt;

&lt;p&gt;那么，这个目标完成情况应该如何衡量？其实很简单，既然\(\vec e\)是一个向量（n维空间），那么\(\vec e\)的长度就是我们需要的指标：&lt;/p&gt;

&lt;p&gt;\[ |\vec e| = \sqrt { \sum ^{n}_{i=1}e_{i}^{2} } \]&lt;/p&gt;

&lt;p&gt;开根号是不必要的，我们可以换成下面这个指标：&lt;/p&gt;

&lt;p&gt;\[ |\vec e|^{2} = \sum ^{n}_{i=1}e_{i}^{2} = \vec e\vec e = \vec e^{T}\vec e \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;小结一下：当\( \vec e^{T}\vec e \)取得最小值时，\(\vec a\)能取得最优解。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;继续推导。&lt;/p&gt;

&lt;p&gt;由上文可知：&lt;/p&gt;

&lt;p&gt;\( \vec e = \vec y - X\vec a \)&lt;/p&gt;

&lt;p&gt;\( \vec e^{T} = (\vec y - X\vec a)^{T} \)&lt;/p&gt;

&lt;p&gt;\( \vec e^{T}\vec e = (\vec y - X\vec a)^{T}(\vec y - X\vec a)  \)&lt;/p&gt;

&lt;p&gt;\( = (\vec y^{T} - \vec a^{T}X^{T})(\vec y - X\vec a)  \)&lt;/p&gt;

&lt;p&gt;\( = \vec y^{T}\vec y - \vec a^{T}X^{T}\vec y - \vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \)&lt;/p&gt;

&lt;p&gt;注意，中间的2个子项是可以合并的。首先，仔细观察\( \vec a^{T}X^{T}\vec y \)这个子项，发现它是一个&lt;strong&gt;值&lt;/strong&gt;，那么就有：&lt;/p&gt;

&lt;p&gt;\( \vec a^{T}X^{T}\vec y  = (\vec a^{T}X^{T}\vec y)^{T} \)&lt;/p&gt;

&lt;p&gt;（一个数值可认为是一个1维的方阵，1维方阵的转置矩阵是它本身）&lt;/p&gt;

&lt;p&gt;而又有：&lt;/p&gt;

&lt;p&gt;\( (\vec a^{T}X^{T}\vec y)^{T} = \vec y^{T}(\vec a^{T}X^{T})^{T} \)&lt;/p&gt;

&lt;p&gt;\( = \vec y^{T}(X\vec a) = \vec y^{T}X\vec a  \)&lt;/p&gt;

&lt;p&gt;得：&lt;/p&gt;

&lt;p&gt;\( \vec a^{T}X^{T}\vec y  = \vec y^{T}X\vec a  \)&lt;/p&gt;

&lt;p&gt;所以上面的方程可变为：&lt;/p&gt;

&lt;p&gt;\[ \vec e^{T}\vec e = \vec y^{T}\vec y - 2\vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \]&lt;/p&gt;

&lt;p&gt;如何让\( \vec e^{T}\vec e \)取得最小值？此时需要使用新的招数：&lt;strong&gt;矩阵微分&lt;/strong&gt;。&lt;/p&gt;

&lt;h2&gt;矩阵微分&lt;/h2&gt;

&lt;p&gt;矩阵微分公式：&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \vec y = A\vec x \]&lt;/p&gt;

&lt;p&gt;y是一个\(m \times 1\)的矩阵，A是一个\(m \times n\)的矩阵，x是一个\(n \times 1\)的矩阵。&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \vec y}{\partial \vec x} = A 【公式1】 \]&lt;/p&gt;

&lt;p&gt;这是如何得到的呢？实际上超级简单，上面这个式子指的是，\(\vec y \)的每一个分量对\(\vec x \)的每一个分量的微分，结果显然就是一个\(m \times n\)矩阵。&lt;/p&gt;

&lt;p&gt;扩展公式：&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \alpha = \vec y^{T}A\vec x \]&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec x} = \vec y^{T}A  【公式2】 \]&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec y} = \vec x^{T}A^{T}  【公式3】 \]&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \alpha = \vec x^{T}A\vec x \]&lt;/p&gt;

&lt;p&gt;且A是对称矩阵，&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec x} = 2\vec x^{T}A  【公式4】 \]&lt;/p&gt;

&lt;h2&gt;应用矩阵微分公式&lt;/h2&gt;

&lt;p&gt;再来看下刚才的\( \vec e^{T}\vec e  \)方程：&lt;/p&gt;

&lt;p&gt;\[ \vec e^{T}\vec e  = \vec y^{T}\vec y - 2\vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \]&lt;/p&gt;

&lt;p&gt;对等号右边的式子求关于\(\vec a\)的微分，得到：&lt;/p&gt;

&lt;p&gt;\( \frac {\partial \vec y^{T}\vec y}{\partial \vec a} - 2\frac {\partial \vec y^{T}X\vec a}{\partial \vec a} + \frac {\partial \vec a^{T}X^{T}X\vec a}{\partial \vec a} \)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当这个式子(导数)等于0时,   就得到了\( \vec e^{T}\vec e \)的最小值。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;显然，第一个子项为0，所以可把它去掉，得到：&lt;/p&gt;

&lt;p&gt;\( - 2\frac {\partial \vec y^{T}X\vec a}{\partial \vec a} + \frac {\partial \vec a^{T}X^{T}X\vec a}{\partial \vec a}  = 0\)&lt;/p&gt;

&lt;p&gt;\(  2\frac {\vec y^{T}X\vec a}{\partial \vec a} = \frac {\vec a^{T}X^{T}X\vec a}{\partial \vec a} \)&lt;/p&gt;

&lt;p&gt;观察左边的式子，和上文的【公式2】是一样的，所以有：&lt;/p&gt;

&lt;p&gt;\( 2\frac {\vec y^{T}X\vec a}{\partial \vec a} = 2\vec y^{T}X \)&lt;/p&gt;

&lt;p&gt;观察右边的式子，符合上文的【公式4】，所以有：&lt;/p&gt;

&lt;p&gt;\(  \frac {\vec a^{T}X^{T}X\vec a}{\partial \vec a} = 2\vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;综上，得：&lt;/p&gt;

&lt;p&gt;\(  2\vec y^{T}X = 2\vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;\(  \vec y^{T}X = \vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;\(  (\vec y^{T}X)^{T} = (\vec a^{T}X^{T}X)^{T} \)&lt;/p&gt;

&lt;p&gt;\(  X^{T}\vec y = X^{T}X\vec a \)&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;这个东西就是所谓的&lt;strong&gt;最小二乘估计(Least Squares Estimator)&lt;/strong&gt;了。&lt;/p&gt;

&lt;h2&gt;特殊情况下的最小二乘估计&lt;/h2&gt;

&lt;p&gt;上文的讨论中没有考虑到一种特殊情况：X是一个方阵。当X是方阵时，上面的公式可进一步简化：&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}(X^{T})^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}\vec y  \]&lt;/p&gt;

&lt;p&gt;还是搞不明白X不是方阵时，公式为什么要那么复杂？答案就在于，X不是方阵时，\( X^{-1} \)和\( (X^{T})^{-1} \)都不成立，导致最小二乘估计公式不能简化。&lt;/p&gt;

&lt;p&gt;关于这个问题的进一步讨论，请阅读我的另一篇文章&lt;a href=&quot;http://www.qiujiawei.com/linear-algebra-16/&quot;&gt;线性代数之伪逆矩阵&lt;/a&gt;。&lt;/p&gt;

&lt;h2&gt;实例&lt;/h2&gt;

&lt;p&gt;使用ML中常用的Iris数据集&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Iris&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Iris&lt;/a&gt;来验证下上面的公式是否可用。&lt;/p&gt;

&lt;p&gt;Iris鸢尾花卉数据集，是一类多重变量分析的数据集。通过&lt;strong&gt;花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性&lt;/strong&gt;预测鸢尾花卉属于（Setosa，Versicolour，Virginica）&lt;strong&gt;三个种类&lt;/strong&gt;中的哪一类。&lt;/p&gt;

&lt;p&gt;整个数据集可在&lt;a href=&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data&quot;&gt;这里&lt;/a&gt;浏览。&lt;/p&gt;

&lt;p&gt;首先是为3个类别各赋予1个标签值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Iris-setosa = 1&lt;/li&gt;
&lt;li&gt;Iris-versicolor = 2&lt;/li&gt;
&lt;li&gt;Iris-virginica = 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后从整个数据集中挑选训练数据集(Train Dataset)，譬如从3个类别中各取出前10个数据项。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此时，已经得到了\( X \)、\( \vec y \)的值了，\( X \)是上面这个表格的前4列（30x4的矩阵），\( \vec y \)是第5列（30x1的矩阵）。&lt;/p&gt;

&lt;p&gt;我们的目标是求出系数\( \vec a \)，它是一个4x1的矩阵。&lt;/p&gt;

&lt;p&gt;根据前文推导出来的公式：&lt;/p&gt;

&lt;p&gt;\(  \vec a = (X^{T}X)^{-1}X^{T}\vec y  \)&lt;/p&gt;

&lt;p&gt;因为矩阵比较庞大的关系，只能直接给出\( (X^{T}X)^{-1}X^{T} \)的结果了，读者们也可以自己做下计算：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;1051.290&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;532.720&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;723.350&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;230.480&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;532.720&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;281.280&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;345.450&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;108.190&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;723.350&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;345.450&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;550.410&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;182.580&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;230.480&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;108.190&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;182.580&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;62.220&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;0.541034&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.573805&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.641245&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.875298&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.573805&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.633743&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.631976&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.830927&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.641245&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.631976&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.920228&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.42389&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.875298&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.830927&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.42389&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;2.396868&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;0.028273&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.206968&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.048125&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.076847&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.083211&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.056253&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.097334&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.032575&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.006168&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.002067&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.162628&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.053786&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.125186&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.228843&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.273287&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.270475&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.098417&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.033124&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.094950&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.011335&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.144267&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.018560&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.174707&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.270956&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.001741&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.003648&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.181042&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.178793&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.046731&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.106397&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.010276&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.191835&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.013523&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.106879&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.131031&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.115039&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.150711&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.067480&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.031694&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.017830&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.181668&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.046873&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.144359&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.250620&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.294553&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.267479&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.117184&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.036068&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.122374&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.028729&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.190919&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.027353&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.189075&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.26628&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.008918&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.033594&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.204029&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.134858&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.090344&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.072185&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.054892&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.242631&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.080010&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.104963&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.072430&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.003185&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.060144&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.038057&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.014794&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.054978&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.134766&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.076454&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.092183&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.243448&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.301346&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.404405&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.092525&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.012496&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.017495&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.032696&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.007320&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.025114&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.217735&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.383162&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.067400&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.105802&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.158253&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.386076&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.057919&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.288185&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.041703&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.282107&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.083251&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.205964&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.12892&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.024128&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.073167&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.105123&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.072449&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.183062&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.131452&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.130738&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.081924&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.323375&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.408249&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.628974&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.084976&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.007234&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.066687&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.110491&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.221148&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.125436&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.354307&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.554733&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.211204&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.204767&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.121187&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.676157&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.157020&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.617249&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.291&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.441&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.637&lt;/span&gt;   
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.094&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;a的值为：&lt;/p&gt;

&lt;p&gt;\[ \vec a =  \left[ \begin{matrix} -0.291\\ 0.441\\ 0.637\\ -0.094\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;然后就是测试a的可靠度如何。方法就是把a用到剩余的其他数据项里，算出predict值，并和实际的值做比较，看预测正确的有多少个，公式为：&lt;/p&gt;

&lt;p&gt;\[ \vec y_{predict} = X_{test}\vec a \]&lt;/p&gt;

&lt;p&gt;结果是：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mf&quot;&gt;0.997&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.103&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.809&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.763&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.822&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.200&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.939&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.923&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.072&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.119&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.992&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.066&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.867&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.007&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.294&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.868&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.026&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.967&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.859&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.044&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.971&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.846&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.241&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.125&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.887&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.702&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.752&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.887&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.852&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.952&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.888&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.505&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.940&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.051&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.364&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.790&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.192&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.946&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.026&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.873&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.563&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.140&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.678&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.366&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.820&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.089&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.419&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.021&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.892&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.854&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;2.583&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.886&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.250&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.341&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.033&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.074&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.182&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.398&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.258&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.623&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.775&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.721&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.874&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.543&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.477&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.470&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.270&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.862&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.183&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.928&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.236&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.346&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.894&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.567&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.114&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.227&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.173&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.092&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.426&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.066&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.580&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.526&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.650&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.441&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.570&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.709&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.766&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.496&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.085&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.268&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.818&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.539&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.074&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.310&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.939&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.969&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.319&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.500&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.742&lt;/span&gt;  
&lt;span class=&quot;mf&quot;&gt;2.772&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.788&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.266&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.733&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.509&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.807&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.752&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.008&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.839&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.465&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.602&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.759&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.392&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.573&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.975&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.902&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.470&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.276&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.556&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.919&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.686&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;再四舍五入一下，得到：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此时就可以算准确率了，经过比较，上面的predict值正确的有109个，总共的测试项有120，准确率高达90.83%哦。&lt;/p&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://economictheoryblog.com/2015/02/19/ols_estimator/&quot;&gt;https://economictheoryblog.com/2015/02/19/ols_estimator/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.atmos.washington.edu/%7Edennis/MatrixCalculus.pdf&quot;&gt;http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 06 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-15/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-15/</guid>
      </item>
    
      <item>
        <title>用线性代数知识解决光线和三角形的交点问题</title>
        <description>&lt;p&gt;本文可认为是《PBRT》3.6节的公式推导笔记。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3&gt;Step1 抽象化问题&lt;/h3&gt;

&lt;p&gt;首先是问题中的3个对象的抽象：&lt;/p&gt;

&lt;p&gt;三角形：由三角形的定义可知，只要确定空间中三个顶点的坐标，就能确定唯一的一个三角形。设三个顶点分别为\(\vec p_{0}\)，\(\vec p_{1}\)，\(\vec p_{2}\)。&lt;/p&gt;

&lt;p&gt;光线：光线在本问题中，设为有起点、有发射方向的射线，起点设为\(\vec o\)，方向为\(\vec d\)。&lt;/p&gt;

&lt;p&gt;光线和三角形的交点：设该交点为\(\vec g\)。&lt;/p&gt;

&lt;p&gt;这时候还要再用到几何数学的一个东西：质心坐标 Barycentric Coordinates。 &lt;a href=&quot;http://mathworld.wolfram.com/BarycentricCoordinates.html&quot;&gt;http://mathworld.wolfram.com/BarycentricCoordinates.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;\[ \vec p_{BC} = b_{0}\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2}\]&lt;/p&gt;

&lt;p&gt;规范化的质心坐标被称为Homogeneous  Barycentric Coordinates，特性是\( b_{0} +  b_{1} + b_{2} = 1 \)，&lt;/p&gt;

&lt;p&gt;因为光线和三角形的交点必然落在三角形内部，所以这个交点可认为是一个\(\vec p_{BC} \)。所以有：&lt;/p&gt;

&lt;p&gt;\[ \vec g = (1 - b_{1} - b_{2})\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;h3&gt;Step2 问题方程&lt;/h3&gt;

&lt;p&gt;因为光线和三角形的交点要么不存在、要么有且只有一个，所以可列出下面的方程：&lt;/p&gt;

&lt;p&gt;\[ \vec o + t \vec d = \vec g  = (1 - b_{1} - b_{2})\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;p&gt;中间的\(\vec g\)去掉：&lt;/p&gt;

&lt;p&gt;\[ \vec o + t \vec d = (1 - b_{1} - b_{2})\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;p&gt;可以看到，只剩下3个未知量：\(t\)、\(b_{1}\)、\( b_{2}\)，它们就是最终要求出来的东西。(注意：这3个都是数，不是向量)&lt;/p&gt;

&lt;p&gt;为了应用线性代数的知识来解决问题，我们需要把这个方程写成线性代数里的线性方程组形式：&lt;/p&gt;

&lt;p&gt;\[ \vec o + t \vec d = (1 - b_{1} - b_{2})\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;p&gt;\[ \vec o + t \vec d = \vec p_{0} - b_{1}\vec p_{0} - b_{2}\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;p&gt;\[ \vec o - \vec p_{0} = -t\vec d - b_{1}\vec p_{0} - b_{2}\vec p_{0} + b_{1}\vec p_{1} + b_{2}\vec p_{2} \]&lt;/p&gt;

&lt;p&gt;\[ -\vec d t + (\vec p_{1}- \vec p_{0}) b_{1} + (\vec p_{2} - \vec p_{0}) b_{2} = \vec o - \vec p_{0} \]&lt;/p&gt;

&lt;p&gt;为了让后续的推导更简洁，需要设：&lt;/p&gt;

&lt;p&gt;\( \vec e_{1} = \vec p_{1} - \vec p_{0} \)&lt;/p&gt;

&lt;p&gt;\( \vec e_{2} = \vec p_{2} - \vec p_{0} \)&lt;/p&gt;

&lt;p&gt;\( \vec s = \vec o - \vec p_{0} \)&lt;/p&gt;

&lt;p&gt;因此，上面的线性方程(组)简化成：&lt;/p&gt;

&lt;p&gt;\[ -\vec d t + \vec e_{1} b_{1} + \vec e_{2} b_{2} = \vec s \]&lt;/p&gt;

&lt;p&gt;再进一步，把方程左边写成矩阵相乘的形式，参数和系数就更显而易见了：&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} -\vec d&amp;amp; \vec e_{1}&amp;amp; \vec e_{2} \\ \end{matrix} \right]  \left[ \begin{matrix} t\\  b_{1}\\  b_{2}\\ \end{matrix} \right] = \vec s \]&lt;/p&gt;

&lt;h3&gt;Step3 解开问题方程&lt;/h3&gt;

&lt;p&gt;接下来使用大招——&lt;strong&gt;克拉默(Cramer)法则&lt;/strong&gt;，来解方程。（在我的&lt;a href=&quot;http://www.qiujiawei.com/linear-algebra/&quot;&gt;&amp;lt;复习向&amp;gt;线性代数之矩阵与行列式(1)&lt;/a&gt; 一文中有介绍）&lt;/p&gt;

&lt;p&gt;克拉默(Cramer)法则：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;若系数行列式 \( D\neq 0 \)，则方程组有唯一解，其解为：
\[ x_{j} = \dfrac {D_{j}} {D} \]&lt;/p&gt;

&lt;p&gt;\( D_{j} \)是将系数行列式D中第j列的元素\( a_{1j},a_{2j},\cdots a_{nj} \)对应地换成方程组右端的常数项\( b_{1j},b_{2j},\cdots b_{nj} \)，而其余各列保持不变得到的行列式。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对应到上面的方程，系数行列式D等于：&lt;/p&gt;

&lt;p&gt;\[ D =    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| \]&lt;/p&gt;

&lt;p&gt;而方程右边的常数项为\(\vec s\)。&lt;/p&gt;

&lt;p&gt;所以上面的方程的各个未知数的值为：&lt;/p&gt;

&lt;p&gt;\( t = \frac {    \left| \begin{matrix} \vec s\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| } {    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| } \)&lt;/p&gt;

&lt;p&gt;\( b_{1} = \frac {    \left| \begin{matrix} -\vec d\  \vec s\  \vec e_{2} \\ \end{matrix} \right| } {    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| } \)&lt;/p&gt;

&lt;p&gt;\( b_{2} = \frac {    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec s \\ \end{matrix} \right| } {    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| } \)&lt;/p&gt;

&lt;p&gt;将三个式子合并下：&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} t\\  b_{1}\\  b_{2}\\ \end{matrix} \right] = \frac { 1 } {    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right| } \left[ \begin{matrix}    \left| \begin{matrix} \vec s\  \vec e_{1}\  \vec e_{2} \\ \end{matrix} \right|  \\    \left| \begin{matrix} -\vec d\  \vec s\  \vec e_{2} \\ \end{matrix} \right|  \\    \left| \begin{matrix} -\vec d\  \vec e_{1}\  \vec s \\ \end{matrix} \right|  \\  \end{matrix} \right]    \]&lt;/p&gt;

&lt;p&gt;然后再使用另一杀招——&lt;strong&gt;向量的混合积&lt;/strong&gt;&lt;a href=&quot;http://mathworld.wolfram.com/ScalarTripleProduct.html&quot;&gt;Scalar Triple Product&lt;/a&gt;，从而再次将上式简化。&lt;/p&gt;

&lt;p&gt;向量的混合积公式：&lt;/p&gt;

&lt;p&gt;\[ [\vec a,\vec b, \vec c] = \vec a\cdot (\vec b \times \vec c) \]&lt;/p&gt;

&lt;p&gt;\[ = \vec b\cdot (\vec c \times \vec a) = \vec b\cdot (-\vec a \times \vec c) \]&lt;/p&gt;

&lt;p&gt;\[ = \vec c\cdot (\vec a \times \vec b) \]&lt;/p&gt;

&lt;p&gt;\[ = det(\vec a \ \vec b \ \vec c) = |\vec a \ \vec b \ \vec c| \]&lt;/p&gt;

&lt;p&gt;因此有：&lt;/p&gt;

&lt;p&gt;\[ |-\vec d \ \vec e_{1} \ \vec e_{2}| = -(-\vec d) \times \vec e_{2})\cdot \vec e_{1} = (\vec d \times \vec e_{2})\cdot \vec e_{1} \]&lt;/p&gt;

&lt;p&gt;\[ |\vec s \ \vec e_{1} \ \vec e_{2}| = (\vec s \times \vec e_{1})\cdot \vec e_{2} \]&lt;/p&gt;

&lt;p&gt;\[ |-\vec d \ \vec s \ \vec e_{2}| = (\vec d \times \vec e_{2})\cdot \vec s \]&lt;/p&gt;

&lt;p&gt;\[ |-\vec d \ \vec e_{1} \ \vec s| = (\vec s \times \vec e_{1})\cdot \vec d \]&lt;/p&gt;

&lt;p&gt;再代入到前面的方程，得到：&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} t\\  b_{1}\\  b_{2}\\ \end{matrix} \right] = \frac { 1 } { (\vec d \times \vec e_{2})\cdot \vec e_{1} }  \left[ \begin{matrix} (\vec s \times \vec e_{1})\cdot \vec e_{2}\\  (\vec d \times \vec e_{2})\cdot \vec s\\  (\vec s \times \vec e_{1})\cdot \vec d\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;再设：&lt;/p&gt;

&lt;p&gt;\[ \vec s_{1} = \vec d \times \vec e_{2} \]&lt;/p&gt;

&lt;p&gt;\[ \vec s_{2} = \vec s \times \vec e_{1} \]&lt;/p&gt;

&lt;p&gt;就得到最终的等式了：&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} t\\  b_{1}\\  b_{2}\\ \end{matrix} \right] = \frac { 1 } { \vec s_{1} \cdot \vec e_{1} }  \left[ \begin{matrix} \vec s_{2}\cdot \vec e_{2}\\  \vec s_{1}\cdot \vec s\\  \vec s_{2}\cdot \vec d\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;推导到了这里就结束了。现在来分析下这个最终等式的特点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;\(\vec d 、\  \vec s 、\  \vec e_{1} 、\  \vec e_{2} \) 是需要先求出来的，不过也是非常容易计算的(向量加减法)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;接着就是算\( \vec s_{1} 、\   \vec s_{2} \)，无法避免的2次叉积运算。算完后，就得到了等式右边所有变量的值了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最后就是4次向量点积运算，1次求倒数运算，3次乘法运算，就分别得到了\( t 、\ b_{1} 、\ b_{2} \)的值。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;总结&lt;/h2&gt;

&lt;p&gt;在上面的推导过程中，用到了矩阵、行列式、向量叉积、向量混合积等诸多概念，只为了得到相交点的坐标，确实复杂了点。&lt;/p&gt;

&lt;p&gt;其中的&lt;strong&gt;向量的混合积&lt;/strong&gt;，可以参考以下网页来理解：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://mathworld.wolfram.com/CrossProduct.html&quot;&gt;http://mathworld.wolfram.com/CrossProduct.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Triple_product&quot;&gt;https://en.wikipedia.org/wiki/Triple_product&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/triangle-intersect/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/triangle-intersect/</guid>
      </item>
    
      <item>
        <title>线性代数之平移、缩放、旋转矩阵</title>
        <description>&lt;!--more--&gt;

&lt;h2&gt;平移矩阵 Translate Matrix&lt;/h2&gt;

&lt;p&gt;\[ T =  \left[ \begin{matrix} 1&amp;amp;0&amp;amp;0&amp;amp;x\\  0&amp;amp;1&amp;amp;0&amp;amp;y\\  0&amp;amp;0&amp;amp;1&amp;amp;z\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h2&gt;缩放矩阵 Scale Matrix&lt;/h2&gt;

&lt;p&gt;\[ S =  \left[ \begin{matrix} x&amp;amp;0&amp;amp;0&amp;amp;0\\  0&amp;amp;y&amp;amp;0&amp;amp;0\\  0&amp;amp;0&amp;amp;z&amp;amp;0\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h2&gt;旋转矩阵 Rotate Matrix&lt;/h2&gt;

&lt;h3&gt;绕(1,0,0)旋转\(\theta \)角度&lt;/h3&gt;

&lt;p&gt;\[ R_{(1,0,0)} =  \left[ \begin{matrix} 1&amp;amp;0&amp;amp;0&amp;amp;0\\  0&amp;amp;cos\theta &amp;amp;sin\theta &amp;amp;0\\  0&amp;amp;-sin\theta &amp;amp;cos\theta &amp;amp;0\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h3&gt;绕(0,1,0)旋转\(\theta \)角度&lt;/h3&gt;

&lt;p&gt;\[ R_{(0,1,0)} =  \left[ \begin{matrix} cos\theta&amp;amp;0&amp;amp;-sin\theta&amp;amp;0\\    0&amp;amp;1&amp;amp;0&amp;amp;0\\  sin\theta &amp;amp;0&amp;amp;cos\theta &amp;amp;0\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h3&gt;绕(0,0,1)旋转\(\theta \)角度&lt;/h3&gt;

&lt;p&gt;\[ R_{(0,0,1)} =  \left[ \begin{matrix} cos\theta&amp;amp;sin\theta&amp;amp;0&amp;amp;0\\  -sin\theta &amp;amp;cos\theta &amp;amp;0&amp;amp;0\\   0&amp;amp;0&amp;amp;1&amp;amp;0\\   0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h3&gt;绕任意轴旋转\(\theta \)角度&lt;/h3&gt;

&lt;p&gt;设旋转轴为\(\vec n\)，这是一个单位化的方向向量。设被旋转的向量为\(\vec v\)，被旋转后是\(\vec v&amp;#39; \)。&lt;/p&gt;

&lt;p&gt;为了求出\(\vec v&amp;#39; \)，需要迂回地处理：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;将\(\vec v\) 分解为 \(\vec v = \vec v_{\perp }+\vec v_{\parallel } \) ，\( \vec v_{\parallel } \)指的是\(\vec v\)与\(\vec n\)平行的部分，\( \vec v_{\perp } \) 指的是\(\vec v\) 与\(\vec n\)垂直的部分。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;分解为两部分后，可以分别对这两个部分做旋转，然后再合并，所以有： \(\vec v&amp;#39; = \vec v&amp;#39;_{\perp }+\vec v&amp;#39;_{\parallel } \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;让 \( \vec v_{\parallel } \) 绕旋转轴\(\vec n\)旋转\(\theta \)角度，它依然保持不变，因为它和\(\vec n\)是同方向的向量，所以有 \( \vec v_{\parallel } = \vec v&amp;#39;_{\parallel } \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据上一点，可以得到： \(\vec v&amp;#39; = \vec v&amp;#39;_{\perp }+\vec v_{\parallel } \)。因此，问题简化为求\( \vec v&amp;#39;_{\perp } \)和\( \vec v_{\parallel } \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;分析\( \vec v_{\parallel } \)，可以发现它相当于是\(\vec v\)在\(\vec n\)上的投影，根据向量的点积公式：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ \vec A\cdot \vec B = |\vec A||\vec B|cos\alpha  \]&lt;/p&gt;

&lt;p&gt;代入\(\vec v\)、\(\vec n\)后，得到：\( \vec v\cdot \vec n = |\vec v||\vec n|cos\alpha = |\vec v|cos\alpha = |\vec v_{\parallel }| \)，即算出了\( \vec v_{\parallel } \)的长度，又因为\vec v_{\parallel } \)和\(\vec n\)方向一致、\(\vec n\)长度为1，所以有:&lt;/p&gt;

&lt;p&gt;\[ \vec v_{\parallel } = (\vec v\cdot \vec n) \vec n \]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;上一步已经解决了\( \vec v_{\parallel } \)，剩下的就是求\( \vec v&amp;#39;_{\perp } \)。求\( \vec v&amp;#39;_{\perp } \)之前需要先求出\( \vec v_{\perp } \)，而显然\( \vec v_{\perp } = v - \vec v_{\parallel} \) &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;接着，需要计算一个新的向量\(\vec w \)，\( \vec w = \vec n \times \vec v_{\perp } \) （注意叉乘的顺序不能错），所以\(\vec w \)是一个垂直于\(  \vec n \)、\( \vec v_{\perp } \)所构成平面的向量。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;把\( \vec v_{\perp }\)、\(\vec w \) 分别当做是\(  \vec n \)、\( \vec v_{\perp } \)平面的x、y轴(2D坐标系)，那么\( \vec v&amp;#39;_{\perp } \)的含义就是指\( \vec v_{\perp } \)在这个2D坐标系下旋转\(\theta \)度。从而得到等式：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ \vec v&amp;#39;_{\perp } =  cos\theta \vec v_{\perp } + sin\theta \vec w \]&lt;/p&gt;

&lt;p&gt;好了，所有变量都得到了，总结下最终的公式：&lt;/p&gt;

&lt;p&gt;\( \vec v_{\parallel } = (\vec v\cdot \vec n) \vec n \)&lt;/p&gt;

&lt;p&gt;\( \vec v_{\perp } = \vec v - \vec v_{\parallel} = \vec v -  (\vec v\cdot \vec n) \vec n \) &lt;/p&gt;

&lt;p&gt;\( \vec w = \vec n \times \vec v_{\perp } \) &lt;/p&gt;

&lt;p&gt;\( = \vec n \times (\vec v - \vec v_{\parallel}) \) &lt;/p&gt;

&lt;p&gt;\( = \vec n \times \vec v - \vec n \times \vec v_{\parallel}) \) &lt;/p&gt;

&lt;p&gt;\( = \vec n \times \vec v \) &lt;/p&gt;

&lt;p&gt;\( \vec v&amp;#39;_{\perp } =  cos\theta \vec v_{\perp } + sin\theta \vec w \)&lt;/p&gt;

&lt;p&gt;\( =  cos\theta (v - (\vec v\cdot \vec n) \vec n) + sin\theta (\vec n \times \vec v)  \)&lt;/p&gt;

&lt;p&gt;\( \vec v&amp;#39; = \vec v&amp;#39;_{\perp } + \vec v_{\parallel } \)&lt;/p&gt;

&lt;p&gt;\( = cos\theta (v - (\vec v\cdot \vec n) \vec n) + sin\theta (\vec n \times \vec v) + (\vec v\cdot \vec n) \vec n \)&lt;/p&gt;

&lt;p&gt;加粗并居中：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;\[ \vec v&amp;#39; = cos\theta (v - (\vec v\cdot \vec n) \vec n) + sin\theta (\vec n \times \vec v) + (\vec v\cdot \vec n) \vec n \]&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这就是绕任意轴的旋转公式了。&lt;/p&gt;

&lt;p&gt;接下来是把这个公式转换成矩阵的形式。方法是，把\(v_{x} = (1,0,0) \)、\(v_{y} = (0,1,0) \)、\(v_{z} = (0,0,1) \)，分别代入上面的公式，分别得到：&lt;/p&gt;

&lt;p&gt;\[ v_{x}&amp;#39; =  \left[ \begin{matrix} n_{x}^{2}(1-cos\theta )+cos\theta \\ n_{x}n_{y}(1-cos\theta )+n_{z}sin\theta \\ n_{x}n_{z}(1-cos\theta )-n_{y}sin\theta \\ \end{matrix} \right] ^{T} \]&lt;/p&gt;

&lt;p&gt;\[ v_{y}&amp;#39; =  \left[ \begin{matrix} n_{x}n_{y}(1-cos\theta )-n_{z}sin\theta \\ n_{y}^{2}(1-cos\theta )+cos\theta \\ n_{y}n_{z}(1-cos\theta )+n_{x}sin\theta \\ \end{matrix} \right] ^{T} \]&lt;/p&gt;

&lt;p&gt;\[ v_{z}&amp;#39; =  \left[ \begin{matrix} n_{x}n_{z}(1-cos\theta )+n_{y}sin\theta \\ n_{y}n_{z}(1-cos\theta )-n_{x}sin\theta \\ n_{z}^{2}(1-cos\theta )+cos\theta \\ \end{matrix} \right] ^{T} \]&lt;/p&gt;

&lt;p&gt;最终的旋转矩阵为:&lt;/p&gt;

&lt;p&gt;\[ R(\vec n, \theta ) =  \left[ \begin{matrix} n_{x}^{2}(1-cos\theta )+cos\theta &amp;amp;  n_{x}n_{y}(1-cos\theta )+n_{z}sin\theta &amp;amp;  n_{x}n_{z}(1-cos\theta )-n_{y}sin\theta &amp;amp;0\\      n_{x}n_{y}(1-cos\theta )-n_{z}sin\theta &amp;amp;n_{y}^{2}(1-cos\theta )+cos\theta &amp;amp;n_{y}n_{z}(1-cos\theta )+n_{x}sin\theta &amp;amp;0\\     n_{x}n_{z}(1-cos\theta )+n_{y}sin\theta &amp;amp;n_{y}n_{z}(1-cos\theta )-n_{x}sin\theta &amp;amp;n_{z}^{2}(1-cos\theta )+cos\theta &amp;amp;0\\    0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-14/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-14/</guid>
      </item>
    
      <item>
        <title>线性代数之视角矩阵Lookat Matrix</title>
        <description>&lt;h1&gt;引言&lt;/h1&gt;

&lt;p&gt;我对视角矩阵的理解是这样子的，假设3维空间有一个观察者（摄像机），这个观察者必然有它的坐标位置、视角、焦点，根据这3个参数，可以建立一个正交化、规范化的坐标系（一个正交化、单位化的3x3矩阵），这个坐标系对应的矩阵就是Lookat矩阵。&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;根据上面这个我自己创造的定义，可以知道，Lookat矩阵只和观察者的坐标、焦点、视角有关，和被观察的东西完全无关，也就是说，Lookat矩阵是independent的，这个性质的好处是，这个观察者的Lookat矩阵，可以应用到任意目标上。&lt;/p&gt;

&lt;p&gt;观察者的坐标、焦点、视角，可以进一步抽象。观察者坐标位置设为eye向量，焦点位置设为focal向量，视角呢，比较特殊，是设为一个up向量，含义是这个观察者的头顶朝向。&lt;/p&gt;

&lt;p&gt;可以想象成，观察者就是你自己，你站在地面上，盯着远处一个美女，可以是盯着她的腿、她的腰，都无妨。&lt;/p&gt;

&lt;p&gt;看的过程中，你可以往左侧着头看，也可以倒立着看，不会影响focal向量，因为你的眼睛还是能看见美女。&lt;/p&gt;

&lt;p&gt;但是当你低头时，情况就有些变化了。第一种情况：你对着美女弯腰90度，focal向量变成指向了地面上的某个点，up向量虽然还是沿着你头部的方向，不过因为弯腰90度的关系，已经不是朝着上方了(大约是指向了美女的方向)；第二种情况是，你只是微微低头(大于15度这样子)，眼睛还是能看到美女（低头15度的视野和完全直立时一致），不过，up向量被改变了，因为你的头转了15度。&lt;/p&gt;

&lt;p&gt;通过这个例子，可以知道，focal向量和up向量之间是存在联系的，而eye向量则和focal、up向量没有关系，eye向量决定的是你所在的位置。&lt;/p&gt;

&lt;p&gt;唠嗑到这里，下面进入数学环节。&lt;/p&gt;

&lt;h1&gt;推导&lt;/h1&gt;

&lt;p&gt;当eye、focal、up三个向量的值确定后，就可以构造Lookat矩阵了。&lt;/p&gt;

&lt;p&gt;(额外补充：focal向量一般是通过计算被观察位置center和观察者的位置eye的差值得到的，focal = center - eye)&lt;/p&gt;

&lt;p&gt;首先明确2点：一，Lookat矩阵是正交且规范化的；二，我们使用的是右手坐标系。&lt;/p&gt;

&lt;p&gt;这个Lookat矩阵，相当于是一个坐标系，那么可以设三个坐标轴的方向向量分别为\(\vec r\)、\(\vec u\)、\(\vec f\)，分别的含义是，观察者坐标系的right、up、forward方向。&lt;/p&gt;

&lt;p&gt;\(\vec f\)可以轻松得到：它的朝向是focal的反方向。为什么呢？很简单，focal是指从观察者位置到焦点位置的方向向量，又因为我们用的是右手坐标系，那么观察者坐标系的f轴朝向当然是focal的反方向了。&lt;/p&gt;

&lt;p&gt;\[\vec f = - \frac {\overrightarrow {focal} }{|\overrightarrow {focal}|} \]&lt;/p&gt;

&lt;p&gt;接着是\(\vec r\)。显然，\(\vec r\)指的方向是，focal和up所构成的平面的垂线的正方向，即focal和up的叉积。&lt;/p&gt;

&lt;p&gt;\[\vec r = \frac {\overrightarrow {focal} \times \overrightarrow {up}}{|\overrightarrow {focal} \times \overrightarrow {up}|} \]&lt;/p&gt;

&lt;p&gt;\(\vec r\)、\(\vec f\)都得到后，\(\vec u\)就简单了，因为\(\vec r\)、\(\vec f\)已经规范化、正交化了的，那么\(\vec u\)就是他们的叉积：&lt;/p&gt;

&lt;p&gt;\[\vec u = \vec f \times \vec r \]&lt;/p&gt;

&lt;p&gt;设Lookat矩阵为M，则M等于：&lt;/p&gt;

&lt;p&gt;\[ M =  \left[ \begin{matrix} r_{x}&amp;amp;r_{y}&amp;amp;r_{z}&amp;amp;0\\  u_{x}&amp;amp;u_{y}&amp;amp;u_{z}&amp;amp;0\\  f_{x}&amp;amp;f_{y}&amp;amp;f_{z}&amp;amp;0\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;拿这个M和单位矩阵I对比下：&lt;/p&gt;

&lt;p&gt;\[ I =  \left[ \begin{matrix} 1&amp;amp;0&amp;amp;0&amp;amp;0\\ 0&amp;amp;1&amp;amp;0&amp;amp;0\\ 0&amp;amp;0&amp;amp;1&amp;amp;0\\ 0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;可以发现，单位矩阵I相当于是把观察者放在世界空间的原点。因为I的\(\vec r\)、\(\vec u\)、\(\vec f\)已经是规范化、正交化的，且和世界坐标系一致。&lt;/p&gt;

&lt;p&gt;所以上面的M可以理解为：M等于M乘以I。含义是，把世界坐标系变换到观察者坐标系。也即相当于调整了对world的观察角度。&lt;/p&gt;

&lt;p&gt;到了这里，事情还没完，因为这个M并不能体现出观察者的&lt;strong&gt;位置&lt;/strong&gt;，为什么呢？因为\(\vec r\)、\(\vec u\)、\(\vec f\)是规范化的向量，长度都为1，并不包含位置信息。&lt;/p&gt;

&lt;p&gt;和单位矩阵I对比的话就清楚了，单位矩阵I之所以不需要位置信息，是因为单位矩阵I已经隐含了一个信息：观察位置就在(0,0,0)。&lt;/p&gt;

&lt;p&gt;观察位置，上面已经定义过了，它就是eye向量。&lt;/p&gt;

&lt;p&gt;把观察者放到eye位置，反过来想，相当于是把被观察的东西偏移-eye的距离。实际上，我们正在构造的Lookat矩阵，不是要作用到观察者身上，而是要作用到被观察者（world）身上的。&lt;/p&gt;

&lt;p&gt;因此，现在可以根据eye向量构造一个移动矩阵T(Translate)了：&lt;/p&gt;

&lt;p&gt;\[ T =  \left[ \begin{matrix} 1&amp;amp;0&amp;amp;0&amp;amp;-eye_{x}\\ 0&amp;amp;1&amp;amp;0&amp;amp;-eye_{y}\\ 0&amp;amp;0&amp;amp;1&amp;amp;-eye_{z}\\ 0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;然后把M和T合并，即得到了Lookat矩阵：&lt;/p&gt;

&lt;p&gt;\[ Lookat = MT \]&lt;/p&gt;

&lt;p&gt;\[ =  \left[ \begin{matrix} r_{x}&amp;amp;r_{y}&amp;amp;r_{z}&amp;amp;0\\  u_{x}&amp;amp;u_{y}&amp;amp;u_{z}&amp;amp;0\\  f_{x}&amp;amp;f_{y}&amp;amp;f_{z}&amp;amp;0\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \left[ \begin{matrix} 1&amp;amp;0&amp;amp;0&amp;amp;-eye_{x}\\ 0&amp;amp;1&amp;amp;0&amp;amp;-eye_{y}\\ 0&amp;amp;0&amp;amp;1&amp;amp;-eye_{z}\\ 0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;\[ =  \left[ \begin{matrix} r_{x}&amp;amp;r_{y}&amp;amp;r_{z}&amp;amp;-r_{x}eye_{x}-r_{y}eye_{y}-r_{z}eye_{z}\\  u_{x}&amp;amp;u_{y}&amp;amp;u_{z}&amp;amp;-u_{x}eye_{x}-u_{y}eye_{y}-u_{z}eye_{z}\\  f_{x}&amp;amp;f_{y}&amp;amp;f_{z}&amp;amp;-f_{x}eye_{x}-f_{y}eye_{y}-f_{z}eye_{z}\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;简化下：&lt;/p&gt;

&lt;p&gt;\[ Lookat =  \left[ \begin{matrix} r_{x}&amp;amp;r_{y}&amp;amp;r_{z}&amp;amp;-(\vec r\cdot \overrightarrow {eye})\\  u_{x}&amp;amp;u_{y}&amp;amp;u_{z}&amp;amp;-(\vec u\cdot \overrightarrow {eye})\\  f_{x}&amp;amp;f_{y}&amp;amp;f_{z}&amp;amp;-(\vec f\cdot \overrightarrow {eye})\\  0&amp;amp;0&amp;amp;0&amp;amp;1\\ \end{matrix} \right] \]&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-13/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-13/</guid>
      </item>
    
      <item>
        <title>线性代数之透视矩阵Perspective Matrix</title>
        <description>&lt;p&gt;本文部分内容翻译自：&lt;a href=&quot;http://www.ogldev.org/www/tutorial12/tutorial12.html&quot;&gt;Tutorial 12: Perspective Projection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文介绍的是OpenGL中的透视矩阵。&lt;/p&gt;

&lt;h1&gt;介绍&lt;/h1&gt;

&lt;p&gt;所谓的透视矩阵，指的是一个“降维”的转换过程。&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;设想下一个在3维空间里的3D模型，它必然拥有一些顶点信息，设其中任意顶点的坐标为(x,y,z,1)（后面的1是齐次坐标的意思），当我们需要把这个模型投影到某个平面上时，它就从3维变成了2维（看过三体3的童鞋就容易理解了，这就是二向箔!），而顶点坐标(x,y,z,1)则变成(x&lt;code&gt;,y&lt;/code&gt;,d,?)。&lt;/p&gt;

&lt;p&gt;可以注意到，经过透视变换后的顶点，依然是四维的形式，只是含义变了，其中的(x&lt;code&gt;,y&lt;/code&gt;)分量指的是这个顶点在投影平面上的坐标(显然是因为投影平面相当于一个2维坐标系)。d指的是这个投影点的深度(depth)，d一般是规范化的，范围是[-1,1]。d的作用在下一个渲染阶段(Depth Test)大有用处。而后面的?，无法一言蔽之，下文会讲到这个问题。&lt;/p&gt;

&lt;h1&gt;视锥体 Frustum&lt;/h1&gt;

&lt;p&gt;视锥体，指的是一个有限的椎体空间，处于这个视锥体里的对象，才是“可见”的对象，可见的对象会被渲染到“视平面”上（三维到二维的投影）。视锥体有4个参数：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;aspect ratio，简称ar，ar = 视平面width/视平面height&lt;/li&gt;
&lt;li&gt;（vertical）field of view，简称fov，指yz平面的视角大小，即下文的\( \alpha \)角。&lt;/li&gt;
&lt;li&gt;near Z Plane，简称near面，是一个平行于xy平面的面，世界坐标系下是一个浮点值，可以用来裁剪太靠近摄像机的物体&lt;/li&gt;
&lt;li&gt;far Z Plane，简称far面，含义类似near面，可以用来裁剪太远离摄像机的物体&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;视平面可以认为是视锥体的near面；far面相对来说并没有那么重要，因为我们知道人眼的“视锥体”是没有far面的（比如裸眼可以看到月亮星星，far面其实是无限远的），在图形学中，far面主要是用来裁剪太过遥远的物体、提高渲染效率的。&lt;/p&gt;

&lt;p&gt;下面这个是我找到的一个视锥体的演示程序，非常直观地展示了视锥体的作用：&lt;/p&gt;

&lt;div&gt;
  &lt;iframe class=&quot;webgl_example&quot; style=&quot;width: 400px; height: 600px;&quot; src=&quot;http://webglfundamentals.org/webgl/frustum-diagram.html&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;http://webglfundamentals.org/webgl/lessons/webgl-3d-perspective.html&quot;&gt;演示程序来源：http://webglfundamentals.org/webgl/lessons/webgl-3d-perspective.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;从摄像机位置（一个点）观察视平面的话，是长这样子的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.3/1.png&quot; alt=&quot;1.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;(图片来自www.ogldev.org)&lt;/p&gt;

&lt;p&gt;y轴范围是[-1,1]，x轴范围是[-ar,ar]，因为ar = 视平面width/视平面height，其实也就是ar=屏幕width/屏幕height，因为大部分屏幕都是宽屏，所以ar的值一般是大于1的。当屏幕宽高一致时，视平面才是上面这幅图的样子。&lt;/p&gt;

&lt;p&gt;现在，换成侧视角来观察这个视锥体(yz平面)：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.3/2.png&quot; alt=&quot;2.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;(图片来自www.ogldev.org，有修改)&lt;/p&gt;

&lt;p&gt;红线是投影面(视平面)，绿色线是摄像机到投影面的向量d，\(\alpha \)角即是fov。注意，OpenGL在“世界坐标系”中，用的是右手坐标系，所以上图中，z轴的左边才是1、右边是-1。因此，向量d的长度是-d（要取反，不然会计算错误）。综上，可以得出：&lt;/p&gt;

&lt;p&gt;\[ tan(\frac {\alpha } { 2 } ) = \frac {1} {|\vec d|} \]&lt;/p&gt;

&lt;p&gt;\[  |\vec d| = -d = \frac {1} { tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;接下来是求某顶点\( (x,y,z,w) \)在投影面上的投影坐标\( (x_{p},y_{p},z_{p},w_{p}) \)。 看下面的侧视图，我们可以先求解\( y_{p} \)：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.3/3.png&quot; alt=&quot;3.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;根据相似三角形定理，可以得到：&lt;/p&gt;

&lt;p&gt;\[ \frac {y_{p} } { |\vec d| } = \frac { y } { -z } \]&lt;/p&gt;

&lt;p&gt;\[ y_{p} = \frac { y * |\vec d| } { -z } = \frac { y } { -z * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;注意，这里的z需要取反，因为上面的等式里，\(y_{p}\)和y同符号，\(|\vec d|\)是正号，而z显然本身是负值，所以z要取反。&lt;/p&gt;

&lt;p&gt;同样的，x分量也可以用相同的公式求得：&lt;/p&gt;

&lt;p&gt;\[ \frac {x_{p} } { |\vec d| } = \frac { x } { -z } \]&lt;/p&gt;

&lt;p&gt;\[ x_{p} = \frac { x * |\vec d| } { -z } = \frac { x } { -z * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;此时要考虑到一个问题： \( y_{p} \)的范围是[-1,1]，而\( x_{p} \)是[-ar, ar]。为了让\( x_{p} \)和\( y_{p} \)一致，需要让\( x_{p} \)除以ar，从而得到：&lt;/p&gt;

&lt;p&gt;\[ x_{p} = \frac { x } { -z * ar * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;\[ y_{p} = \frac { y } { -z * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;到了这里，我们可以开始构造下透视矩阵了：&lt;/p&gt;

&lt;p&gt;\[ Perspective Matrix = M =  \left[ \begin{matrix} a&amp;amp;b&amp;amp;c&amp;amp;d\\ e&amp;amp;f&amp;amp;g&amp;amp;h\\ i&amp;amp;j&amp;amp;k&amp;amp;l\\ m&amp;amp;n&amp;amp;o&amp;amp;p\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;被转换的顶点的坐标(矩阵)是：&lt;/p&gt;

&lt;p&gt;\[ V =  \left[ \begin{matrix} x\\ y\\ z\\ w\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;转换后的投影点是:&lt;/p&gt;

&lt;p&gt;\[ V_{p} =  \left[ \begin{matrix} x_{p} \\ y_{p} \\ z_{p} \\ w_{p} \\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;转换过程:&lt;/p&gt;

&lt;p&gt;\[ MV = V_{p} \]&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} a&amp;amp;b&amp;amp;c&amp;amp;d\\ e&amp;amp;f&amp;amp;g&amp;amp;h\\ i&amp;amp;j&amp;amp;k&amp;amp;l\\ m&amp;amp;n&amp;amp;o&amp;amp;p\\ \end{matrix} \right]  \left[ \begin{matrix} x\\ y\\ z\\ w\\ \end{matrix} \right]  =  \left[ \begin{matrix} x_{p} \\ y_{p} \\ z_{p} \\ w_{p} \\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;从以上等式可以得到:&lt;/p&gt;

&lt;p&gt;\[ ax + by + cz + dw = x_{p} = \frac { x } {  -z * ar * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;这是M矩阵第一行和V的点积等式。求解这个等式的话，会发现可以让b=0、d=0，从而等式简化成:&lt;/p&gt;

&lt;p&gt;\[ ax + cz = \frac { x } { -z * ar * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;这样做后就有了个问题：找不到可以代入a、c的常量值。其中左边比较多余的cz项，如果可以干掉的话，意味着c必须等于0。这么做后，等式进而变成:&lt;/p&gt;

&lt;p&gt;\[ ax = \frac { x } { -z * ar * tan(\frac {\alpha } { 2 } ) } \]&lt;/p&gt;

&lt;p&gt;观察等式，可以发现等式右边有个多余的z。OpenGL中对这个问题的处理是，在变换过程中强(偷)制(偷)插入一个步骤：把矩阵相乘的结果值再统一除以-z。对，没错，确实是-z而不是z，负号的作用是把坐标从右手坐标系转换到左手坐标系，原因是NDC(Normalized Device Coord)坐标系是左手坐标系，即NDC的z轴的正方向是朝向屏幕里面的。这个除以-z的技巧被称为&lt;strong&gt;Perspective Divide&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;这么做之后，事情就简单了，上面的等式可以推出：&lt;/p&gt;

&lt;p&gt;\[ a = \frac { 1 } { ar * tan(\frac {\alpha } { 2 } ) }  \]&lt;/p&gt;

&lt;p&gt;对于M矩阵的f，用同样的做法可以得到:&lt;/p&gt;

&lt;p&gt;\[ f = \frac { 1 } { tan(\frac {\alpha } { 2 } ) }   \]&lt;/p&gt;

&lt;p&gt;从而得到了M的前两行的值：&lt;/p&gt;

&lt;p&gt;\[ M =  \left[ \begin{matrix} \frac { 1 } { ar * tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0&amp;amp;0\\ 0&amp;amp;\frac { 1 } { tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0\\ i&amp;amp;j&amp;amp;k&amp;amp;l\\ m&amp;amp;n&amp;amp;o&amp;amp;p\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;到了这里，其实透视变换问题已经解决大半了，因为\(x_{p}\)和\(y_{p}\)都可以算了，并且可以规范化到[-1,1]范围。剩下的问题是\(z_{p}\)，即顶点的深度信息。&lt;/p&gt;

&lt;p&gt;前面提到的&lt;strong&gt;Perspective Divide&lt;/strong&gt;会导致一个问题：z分量在转换过程中会因为&lt;strong&gt;Perspective Divide&lt;/strong&gt;而导致变成-1。针对这个问题，OpenGL的解决方案是，把V的z值取反并复制覆盖到w上，从而把原始z值保存起来（也就是M矩阵的第四行所负责的事情），同时&lt;strong&gt;Perspective Divide&lt;/strong&gt;仅对x、y、z有效（跳过w）。&lt;/p&gt;

&lt;p&gt;因此，M的后两行也可以得到了：&lt;/p&gt;

&lt;p&gt;\[ M =  \left[ \begin{matrix} \frac { 1 } { ar * tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0&amp;amp;0\\ 0&amp;amp;\frac { 1 } { tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0\\ 0&amp;amp;0&amp;amp;0&amp;amp;0\\ 0&amp;amp;0&amp;amp;-1&amp;amp;0\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;然而，事情还没有结束。现在用这个新的M去做透视变换后，得不到规范化的z分量。规范化的，可以使得后续的渲染步骤不需要知道near Z和far Z。为了完成这个事情，需要对M做改进，着手点就是row 3，全为0的第三行。&lt;/p&gt;

&lt;p&gt;再阐述一下问题：我们需要求出row3=(i,j,k,l)，使得row3和V做点积运算能得到规范化的\(z_{p}\)。用公式表示：&lt;/p&gt;

&lt;p&gt;\[z_{p} = Az + B , z_{p}\in [-1,1] \]&lt;/p&gt;

&lt;p&gt;再考虑上&lt;strong&gt;Perspective Divide&lt;/strong&gt;，上式变成：&lt;/p&gt;

&lt;p&gt;\[z_{p} = A + \frac {B}{-z} , z_{p}\in [-1,1] \]&lt;/p&gt;

&lt;p&gt;思路非常明确了：把公式中的A、B求出来，代入row3，就能解决问题。&lt;/p&gt;

&lt;p&gt;因为当z等于near Z时，\(z_{p}\)必然等于1；当z等于far Z时，\(z_{p}\)必然等于-1 (Note：这里用的是右手坐标系）。因此得到：&lt;/p&gt;

&lt;p&gt;\[ A + \frac {B}{-NearZ} = 1 \]&lt;/p&gt;

&lt;p&gt;\[ A = 1 - \frac {B}{-NearZ} = 1 + \frac {B}{NearZ}  \]&lt;/p&gt;

&lt;p&gt;接着：&lt;/p&gt;

&lt;p&gt;\[ A + \frac {B}{-FarZ} = -1 \]&lt;/p&gt;

&lt;p&gt;\[ 1 + \frac {B}{NearZ} - \frac {B}{FarZ} = -1 \]&lt;/p&gt;

&lt;p&gt;\[ \frac {B*FarZ - B*NearZ}{NearZ*FarZ} = -2 \]&lt;/p&gt;

&lt;p&gt;\[ B = \frac {-2*NearZ*FarZ}{FarZ - NearZ} = \frac {2*NearZ*FarZ}{NearZ - FarZ}  \]&lt;/p&gt;

&lt;p&gt;B解决了，求A：&lt;/p&gt;

&lt;p&gt;\[ A = 1 + \frac {B}{NearZ}  = 1 + \frac {2*FarZ*NearZ}{NearZ*(NearZ - FarZ)} \]&lt;/p&gt;

&lt;p&gt;\[ A = 1 + \frac {2*FarZ}{NearZ - FarZ}  \]&lt;/p&gt;

&lt;p&gt;\[ A =\frac {NearZ - FarZ + 2*FarZ}{NearZ - FarZ}\]&lt;/p&gt;

&lt;p&gt;\[ A = \frac {NearZ + FarZ}{NearZ - FarZ} \]&lt;/p&gt;

&lt;p&gt;有了A、B后，就可以求row3了:&lt;/p&gt;

&lt;p&gt;\[ ix +jy +kz +lw = Az + B \]&lt;/p&gt;

&lt;p&gt;显然，可让i = j = 0，那么上式变成:&lt;/p&gt;

&lt;p&gt;\[ kz + lw = Az + B \]&lt;/p&gt;

&lt;p&gt;因为V的w分量必然是1，所以可以得知：k = A，l = B。&lt;/p&gt;

&lt;p&gt;代入M，得到最终完善的M：&lt;/p&gt;

&lt;p&gt;\[ M =  \left[ \begin{matrix} \frac { 1 } { ar * tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0&amp;amp;0\\ 0&amp;amp;\frac { 1 } { tan(\frac {\alpha } { 2 } ) }&amp;amp;0&amp;amp;0\\ 0&amp;amp;0&amp;amp;\frac {NearZ + FarZ}{NearZ - FarZ}&amp;amp;\frac {2*FarZ*NearZ}{NearZ - FarZ}\\ 0&amp;amp;0&amp;amp;-1&amp;amp;0\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;再对比下superbible7中构造透视矩阵的代码：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mat4&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;perspective&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fovy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;radians&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fovy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aspect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;mat4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;仔细观察，发现有1处不同：这个函数构造的矩阵是列主导的矩阵。其中元素的取值和本文的推导完全一致！&lt;/p&gt;
</description>
        <pubDate>Sun, 13 Mar 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-12/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-12/</guid>
      </item>
    
      <item>
        <title>《OpenGL编程指南(第8版)》阅读笔记01</title>
        <description>&lt;h1&gt;Example源码Bug备忘&lt;/h1&gt;

&lt;h2&gt;第三章&lt;/h2&gt;

&lt;h3&gt;ch03_drawcommands&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;    &lt;span class=&quot;n&quot;&gt;glUniformMatrix4fv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_model_matrix_loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GL_FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;应改为：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;    &lt;span class=&quot;n&quot;&gt;glUniformMatrix4fv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render_model_matrix_loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GL_FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;原因：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;void glUniformMatrix4fv(GLint location, GLsizei count, GLboolean transpose, const GLfloat *value);&lt;/p&gt;

&lt;p&gt;Parameters&lt;/p&gt;

&lt;p&gt;location&lt;/p&gt;

&lt;p&gt;Specifies the location of the uniform value to be modified.&lt;/p&gt;

&lt;p&gt;count&lt;/p&gt;

&lt;p&gt;Specifies the number of matrices that are to be modified. This should be 1 if the targeted uniform variable is &amp;gt; not an array of matrices, and 1 or more if it is an array of matrices.&lt;/p&gt;

&lt;p&gt;transpose&lt;/p&gt;

&lt;p&gt;Specifies whether to transpose the matrix as the values are loaded into the uniform variable. Must be GL_FALSE.&lt;/p&gt;

&lt;p&gt;value&lt;/p&gt;

&lt;p&gt;Specifies a pointer to an array of count values that will be used to update the specified uniform variable.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;因为例子使用的是primitive_restart.vs.glsl顶点着色器，model_matrix不是数组，所以参数count应该为1。&lt;/p&gt;

&lt;p&gt;还发现了一句注释： // &amp;quot;model_matrix&amp;quot; is actually an array of 4 matrices&lt;/p&gt;

&lt;p&gt;不明白为什么model_matrix会是一个包含4个矩阵的数组。并且确实改成1后就能运行了。&lt;/p&gt;

&lt;p&gt;(后面发现有一个shader里面的model_matrix确实是一个长度4的数组...估计是混淆了吧)&lt;/p&gt;

&lt;p&gt;真是坑。&lt;/p&gt;

&lt;h3&gt;vmath&lt;/h3&gt;

&lt;p&gt;vmath::rotation 要改为 vmath::rotate
vmath::translation 要改为 vmath::translate&lt;/p&gt;

&lt;h3&gt;绝对路径....&lt;/h3&gt;

&lt;p&gt;ch03_instancing的Initialize函数里：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;    &lt;span class=&quot;c1&quot;&gt;// Load the object&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LoadFromVBM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;C:/Vermilion-Book/trunk/Code/media/armadillo_low.vbm&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;太坑了。&lt;/p&gt;

&lt;h3&gt;把vs和fs写在cpp文件里&lt;/h3&gt;

&lt;p&gt;如果是为了演示如何手动编译shader，也不用放在多个example里吧。&lt;/p&gt;

&lt;h2&gt;第四章&lt;/h2&gt;

&lt;h3&gt;ch04_shadowmap的vbm.h和vbm.cpp是不对的，运行会出错&lt;/h3&gt;

&lt;p&gt;换成第三章的工程里的就OK了。&lt;/p&gt;
</description>
        <pubDate>Sun, 21 Feb 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/openl-01/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/openl-01/</guid>
      </item>
    
      <item>
        <title>我的2015总结</title>
        <description>&lt;!--more--&gt;

&lt;h1&gt;Career&lt;/h1&gt;

&lt;p&gt;2015年是我career发生大改变的一年。&lt;/p&gt;

&lt;p&gt;毕业后我在某益工作了接近两年。在2015年春季，我选择了离去。&lt;/p&gt;

&lt;p&gt;为什么要走呢?说大了讲，是个人和公司价值观的不同，说小了讲，是对这种生活的厌倦。&lt;/p&gt;

&lt;p&gt;做这个决定并不轻松。家里人和女朋友其实是不喜欢我跳槽的，个中原因主要是收入。钱、时间、理想，这3个东西太难权衡，本文最后再分享下我对它们的思考。&lt;/p&gt;

&lt;p&gt;走的时候是4月份，走之前，我完成了项目的战斗系统的构建，算是有始有终；走之后，这项目也如预期一样，砍掉了。&lt;/p&gt;

&lt;p&gt;2年时间，参与的项目一个个地因为各种和我个人没有关系的原因而失败，深深体会到作为普通软件工程师的无力感。为了改变现状，我想我必须去做一些和以往不同的事情。如何实施？这是我2015年下旬的思考内容。&lt;/p&gt;

&lt;p&gt;2015下旬，我进入了一家创业公司：壕游戏。整个公司包括我只有10个人左右。目前，一切安好。&lt;/p&gt;

&lt;p&gt;虽然这是一家创业公司，但是没有当年刚加入某益时的那种压力。公司一切相对轻松、自由，每个员工都可以畅所欲言，即使是程序员也能对项目提出建议。这也是我认为的正确的企业文化之一。然而，这种轻松的氛围却容易导致员工的松懈。我下了决心换了工作，并不是为了放松、享乐，我是为了改变现状而离开某益。我不安，因周围的同学跑得太快，生怕懒惰而落下，且我也没有在20多岁就开始享乐的资本。&lt;/p&gt;

&lt;p&gt;所以，我接受了工作上的任何挑战。这半年时间，我给公司开发了一套游戏服务器系统，做各种小优化工具，解决各种疑难问题，并独立负责3个项目的开发(实际上有2个已经因为某些原因而搁置)。做的事情还算不少吧。我也顺利地从python阵营转入js阵营。js和c++将是我今后在编程上的两把重要兵器。&lt;/p&gt;

&lt;p&gt;新工作因为不用加班，给了我更多的时间做自己的事情，很多计划都得以实施，看书的时间变多，也有时间写blog、学英语。甚至还学会了做各种中式饭菜= =。&lt;/p&gt;

&lt;p&gt;这半年时间，我也对自己今后的career有了新的决策，以下是todolist(也是2016年的计划)：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;努力提高技术实力。我相信个人技术的提升，在达到一定程度时会对我的career有重大影响。打算在2016年逐步把以前半懂半不懂的东西都给研究清楚，包括图形学、机器学习等我感兴趣的学科的数学、算法基础知识。搞懂了原理后，要做什么开发都会更得心应手。具体做法是，看书、写代码实践。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;写博客。作为engineer，怎么可以没有一个自己的技术博客呢。写博客这个事情，我觉得有不少好处：巩固学到的知识、方便日后复习知识、锻炼写作表达能力、方便个人marketing(目前还没有效果)。百利而无一害，除了打字比较费功夫(把娱乐节目砍掉就抵消了)。实际上，我写博客是从大二开始的，期间写写停停，陆陆续续换了几个博客系统，从cppblog第三方博客到github静态博客，然而毕业后因为工作繁忙，完全荒废了。辞职前，我开始捡回我“写博客的梦想”。辞旧迎新，我把旧的博客git整个删了(旧博文现在看来，太无聊了)，同时也因为那个博客的框架代码是大三时借鉴其他博主的代码弄出来的，不是自己的代码，难以维护。现在我改用jekyll-now来搭建博客，更简单易用。也因为2014年下半年的驻梦项目，我的web开发实力有了大大的提高，新博客在技术上也没什么隐患了。目前新博客已经持续写了半年。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;提高英语能力。“听”会通过听有声书提高，“读”和“说”通过阅读英文文章(技术书、Quora等)、朗读英文小说来提高。我在EF的学习也即将步入尾声。总的来说，这段经历使得我对语言学习产生了更大的兴趣和自信，是一个好的转折点。但之后应该不会再花大钱在外语学习上了。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;Life&lt;/h1&gt;

&lt;p&gt;2015年除了过年时去了杭州，由于工作上的不顺心(其实是没钱)，之后都没有外出旅行。2015开始，更重视的是积蓄的问题，因为我发现工作2年后，依然存不到多少钱，对建立家庭来说，远远不足。在中国白手起家想要做到安居乐业，有车有房，完全就是一个Hard难度的游戏。当然，游戏虽难，但仍有希望。&lt;/p&gt;

&lt;p&gt;2015年，家庭关系比较和谐，没出啥状况。和GF也还行，有波澜但终归和睦。2014年年底我们一起通宵打Minecraft，那时候她是一名见习医生；2015年年底我们一起通宵打Don&amp;#39;t Starve Together，这时候，她已经走在职业转型的道路上，即将成为一名美术生。也许以后我们会一起做独立游戏。可能这就是destiny吧。&lt;/p&gt;

&lt;p&gt;对于物质生活，我是有期待的，但是暂时无法提高，也就不勉强。其它不实际的期待也没有了，只但愿这副身子骨不散架，平平安安，就好。&lt;/p&gt;

&lt;p&gt;存钱的问题，其实就是买房结婚生孩子的问题，连这个问题都搞不定，事业就不算成功。2016年如无意外，也不能够解决这个问题。现在唯一能做的事情只是投资自己、好好学习。通过游戏开发以外的方法赚钱，也难。&lt;/p&gt;

&lt;h1&gt;My Philosophy Changed&lt;/h1&gt;

&lt;p&gt;最近和朋友分享过我的哲学观:一个我自己都认为不对的效益主义(utilitarianism)。凡事衡量、计算得失，择优而行。然而，这个方法不总是有效，因为有些事情，做不做，无法立即得到回报，相当于长远的、有一定风险的投资。譬如我可以花很多时间精力去广交朋友，做个人marketing，然而我无法确定这对我是不是一件好事。我现在的哲学观是：喜欢就做，不喜欢就不做。这可能无法得到最优解，但起码可以让我活得不像是个只会计算得失的机器。有时候想太多真的没用。&lt;/p&gt;

&lt;p&gt;至于钱、时间、理想，我是这样看的：没钱的时候不能一味追求理想，应该先求生存再求发展；没钱的时候可以用时间换钱(去打工，哪怕是派传单)；有钱的时候用钱买时间(譬如回家不坐火车坐飞机，买高配的机器提高开发效率节省时间)。也即是说，要根据当下条件，做相应的决策。&lt;/p&gt;

&lt;p&gt;所以我觉得我现在，是最适合追求理想的，因未成家，没有太多开销，时间基本都是自己的(有一定的时间)，同时收入也足够维持日常开销(有一定的钱)。怕是再过几年，再追求什么大理想，就更艰难了。&lt;/p&gt;

&lt;p&gt;珍惜当下。&lt;/p&gt;

&lt;p&gt;Goodbye 2015。&lt;/p&gt;
</description>
        <pubDate>Wed, 06 Jan 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/summarize/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/summarize/</guid>
      </item>
    
  </channel>
</rss>