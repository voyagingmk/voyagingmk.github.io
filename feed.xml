<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wyman的技术博客</title>
    <description>博主主要学习方向：图形学、机器学习，以及各种有趣的数学。联系QQ：234707482。</description>
    <link>http://www.qiujiawei.com</link>
    <atom:link href="http://www.qiujiawei.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>记录c++一些神奇的报错</title>
        <description>&lt;!--more--&gt;

&lt;h2&gt;Heap Corruption Deteched&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/10.png&quot; alt=&quot;10.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;这个报错发生在main函数返回时。出错原因是在堆数组的赋值上。&lt;/p&gt;

&lt;p&gt;先是初始化一个数组指针：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后循环赋值：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;注意，这个循环溢出了，这个循环对array[100]进行了赋值，即数组的第101个元素，而数组长度只有100。&lt;/p&gt;

&lt;p&gt;然后再执行delete[] array就会出上面截图那个报错了。&lt;/p&gt;

&lt;p&gt;修了那个循环次数就没事了，就酱。&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/c++-error-solved/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/c++-error-solved/</guid>
      </item>
    
      <item>
        <title>渲染基础理论的介绍</title>
        <description>&lt;!--more--&gt;

&lt;h1&gt;基础概念&lt;/h1&gt;

&lt;h2&gt;辐射通量(Flux)&lt;/h2&gt;

&lt;p&gt;辐射通量(Radiant Flux)，指的是单位时间到达一块平面(或一个局部空间区域)的能量总和。单位是焦耳每秒(joules/second,，J/s)，或瓦特(watts，W)。符号是\(\Phi \)。&lt;/p&gt;

&lt;p&gt;一个点光源发射出去的能量大小可以用Flux来描述。其中要注意的是，Flux描述的是单位时间的能量，那么对于点光源来说，Flux只和光源的强弱有关，所以下图的2个圆圈的Flux值是一样的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/4.png&quot; alt=&quot;4.png&quot;&gt;&lt;/p&gt;

&lt;h2&gt;辐射密度(Irradiance) 和 辐射照度(Radiant Exitence)&lt;/h2&gt;

&lt;p&gt;辐射密度也叫辐射照度。定义了辐射通量后，就可以定义辐射照度了，辐射照度指的是单位面积&lt;strong&gt;进入&lt;/strong&gt;的辐射通量，单位是\(W/m^{2}\)。根据这个定义用符号E表示。&lt;/p&gt;

&lt;p&gt;辐射照度和辐射密度是近似的东西，辐射照度指的是单位面积&lt;strong&gt;离开&lt;/strong&gt;的辐射通量，单位也是\(W/m^{2}\)。用符号M表示。&lt;/p&gt;

&lt;p&gt;以上面的点光源来分析，可以知道上图中内圆圈的辐射照度比外圆圈的辐射照度大，这是因为内圆圈的面积更小而点光源的Flux值恒定，所以内圆圈的E值就大。&lt;/p&gt;

&lt;p&gt;用公式表示：&lt;/p&gt;

&lt;p&gt;\[ E = \frac { 点光源辐射通量 }{ 球的表面积 } =  \frac {\Phi}{4\pi r^{2} } \]&lt;/p&gt;

&lt;p&gt;可见，W恒定，半径r越小，那么辐射照度E越大。&lt;/p&gt;

&lt;p&gt;当假设光源在无限远处时，可把光源认为是一块平面（这种光源叫方向光）。此时，光源平面与被照射平面存在2种情形：光源平面与被照射平面平行（下图中的A）、光源平面与被照射平面不平行（下图中的B）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/5.png&quot; alt=&quot;5.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;(图中的平面附近的A指的是面积Area)&lt;/p&gt;

&lt;p&gt;当光源平面与被照射平面平行时，有：&lt;/p&gt;

&lt;p&gt;\[ E_{1} = \frac {\Phi}{ A } \]&lt;/p&gt;

&lt;p&gt;当光源平面与被照射平面不平行时，需要根据平面的法向量和光线方向的夹角θ，先求出\( A^{&amp;#39;} \)：&lt;/p&gt;

&lt;p&gt;\[ cos\theta = \frac { A }{ A^{&amp;#39;} }  \]&lt;/p&gt;

&lt;p&gt;\[ A^{&amp;#39;}= \frac { A }{ cos\theta }  \]&lt;/p&gt;

&lt;p&gt;于是得到：&lt;/p&gt;

&lt;p&gt;\[ E_{2} = \frac {\Phi}{ A^{&amp;#39;} } =  \frac {\Phi}{ \frac { A }{ cos\theta }  } =  \frac {  \Phi  cos\theta  }{ A } \]&lt;/p&gt;

&lt;p&gt;也可以记为&lt;/p&gt;

&lt;p&gt;\[ E = \frac { \Phi  cos\theta  }{ A^{\perp } }  \]&lt;/p&gt;

&lt;p&gt;( \(  A^{\perp } \) 指A&amp;#39;在光线的方向的正交平面上的投影)&lt;/p&gt;

&lt;p&gt;微分形式：&lt;/p&gt;

&lt;p&gt;\[ dE = \frac {  d\Phi  cos\theta  }{ dA^{\perp }  } \]&lt;/p&gt;

&lt;p&gt;根据这个式子，可以想到，当θ逼近0度时，cosθ等于1，法向量和光线方向平行（上图中的A）；当θ逼近90度时，cosθ等于0，辐射照度E为0（光线垂直于法向量了）。&lt;/p&gt;

&lt;h2&gt;立体角(Solid Angle)&lt;/h2&gt;

&lt;p&gt;立体角的介绍请访问：&lt;a href=&quot;http://www.qiujiawei.com/solid-angle/&quot;&gt;立体角(Solid Angle)详解&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;辐射亮度(Radiance)&lt;/h2&gt;

&lt;p&gt;辐射亮度是指辐射通量与单位面积(注意，是与光线方向正交的那块)单位立体角的比值。符号为L。定义式如下：&lt;/p&gt;

&lt;p&gt;\[ L = \frac { d\Phi }{ d\omega dA^{\perp } } \]&lt;/p&gt;

&lt;p&gt;或：&lt;/p&gt;

&lt;p&gt;\[ L = \frac { I }{  dA^{\perp } }  \]&lt;/p&gt;

&lt;p&gt;物理含义如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/7.png&quot; alt=&quot;7.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;注意：在计算机图形学中，辐射亮度比起上面其他物理量，都重要地得多。&lt;/p&gt;

&lt;p&gt;如果要求平面上某点p的某方向\(\omega \)的辐射亮度L(Radiance)，可用下面的符号表示：&lt;/p&gt;

&lt;p&gt;\[ L(p,\omega ) \]&lt;/p&gt;

&lt;p&gt;其中，\(\omega \)的方向需要注意，因为它是一个立体角，立体角的圆心是p，\(\omega \)的朝向必然是从圆心p往外（向量起点是p）。&lt;/p&gt;

&lt;p&gt;实际上，需要区分成入射(input)和出射(output)2种辐射亮度L，用下面2个符号表示：&lt;/p&gt;

&lt;p&gt;\[ L_{i}(p,\omega ) \]&lt;/p&gt;

&lt;p&gt;\[ L_{o}(p,\omega ) \]&lt;/p&gt;

&lt;p&gt;且在现实世界中有：&lt;/p&gt;

&lt;p&gt;\[ L_{i}(p,\omega ) \neq L_{o}(p,\omega ) \]&lt;/p&gt;

&lt;p&gt;还有，上面的这个p不能简单认为真的是一个无体积的点，它也可能是一个无限小的平面块，即它是一个有面积A、有法向量n的“点”。对于这样一个“点”，我们可以求出它的上半球(沿着n的方向)的辐射密度值\( E(p, n) \)：&lt;/p&gt;

&lt;p&gt;\[ E(p, n) = \int _{\Omega } L_{i} (p,\omega ) |cos\theta |d\omega \]&lt;/p&gt;

&lt;p&gt;分析下这个式子的由来。首先搬出上文给出的L和E的公式：&lt;/p&gt;

&lt;p&gt;\[ L = \frac { d\Phi }{ d\omega dA^{\perp } } \]&lt;/p&gt;

&lt;p&gt;\[ E = \frac {  d\Phi  cos\theta  }{ dA^{\perp }  } \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ d\Phi = L d\omega dA^{\perp } \]&lt;/p&gt;

&lt;p&gt;\[ 
E = \frac {  d\Phi  cos\theta  }{ dA^{\perp }  }
= \frac {  L d\omega dA^{\perp }  cos\theta  }{ dA^{\perp }  }&lt;br&gt;
= L d\omega cos\theta \]&lt;/p&gt;

&lt;p&gt;对上式做整个半球的积分，就得到了：&lt;/p&gt;

&lt;p&gt;\[ E = \int _{\Omega }L|cos\theta |d\omega \]&lt;/p&gt;

&lt;p&gt;也就是：&lt;/p&gt;

&lt;p&gt;\[ E(p, n) = \int _{\Omega } L_{i} (p,\omega ) |cos\theta |d\omega \]&lt;/p&gt;

&lt;p&gt;其中的\( cos\theta \)加绝对值是因为我们求的是半球的积分，立体角\(\omega \)和法向量的夹角必然是锐角，锐角的余弦值必然大于等于0。&lt;/p&gt;

&lt;p&gt;如果把式子中的\(d\omega \)替换成球形角(Sphere Angle)，则得到：&lt;/p&gt;

&lt;p&gt;\[ E(p, n) = \int _{\Omega } L_{i} (p,\omega ) |cos\theta |sin\theta d\theta d\phi \]&lt;/p&gt;

&lt;p&gt;这个式子是不对的，因为积分那里用了立体角，需要将其转换成对\(\theta 和 \phi \)的积分。因为这里积分的是半球，那么\(\theta \)的取值范围是\( [0,\frac {π}{2}] \)、\(\phi \)的取值范围是\( [0,2π] \)：&lt;/p&gt;

&lt;p&gt;\[ E(p, n) = \int _{0 }^{ 2π } \int _{0 }^{ \frac {π}{2} }  L_{i} (p,\theta ,\phi ) cos\theta sin\theta d\theta d\phi \]&lt;/p&gt;

&lt;p&gt;（因为已经明确限定了\(\theta \)的取值范围，所以\( cos\theta \)必然大于等于0，可去掉绝对值符号）&lt;/p&gt;

&lt;p&gt;如果\(L_{i} (p,\theta ,\phi ) \)是一个常量值，那么就意味着任意方向的Radiance都是相等的，于是上式可以求出积分：&lt;/p&gt;

&lt;p&gt;\[ E(p, n) = L_{i} (p,\theta ,\phi ) \int _{0 }^{ 2π } \int _{0 }^{ \frac {π}{2} }  cos\theta sin\theta d\theta d\phi \]&lt;/p&gt;

&lt;p&gt;\[ = L_{i} (p,\theta ,\phi ) \int _{0 }^{ 2π } (\frac {1}{2}sin^{2}\theta )\rvert ^{\frac {π}{2}}_{0} d\phi \]&lt;/p&gt;

&lt;p&gt;\[ = L_{i} (p,\theta ,\phi ) \int _{0 }^{ 2π } (\frac {1}{2}sin^{2}\frac {π}{2} - \frac {1}{2}sin^{2}0  ) d\phi \]&lt;/p&gt;

&lt;p&gt;\[ = L_{i} (p,\theta ,\phi ) \int _{0 }^{ 2π } \frac {1}{2} d\phi \]&lt;/p&gt;

&lt;p&gt;\[ = L_{i} (p,\theta ,\phi ) \frac {1}{2}( 2π - 0) \]&lt;/p&gt;

&lt;p&gt;\[ = L_{i} (p,\theta ,\phi ) π  \]&lt;/p&gt;
</description>
        <pubDate>Sun, 10 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/rendering-equation/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/rendering-equation/</guid>
      </item>
    
      <item>
        <title>立体角(Solid Angle)详解</title>
        <description>&lt;p&gt;理解立体角之前要先理解圆心角。在二维平面上，一个圆的圆弧的微分记为ds(也叫弧微分)，半径为r，则圆心角指的是弧微分与半径的比值:&lt;/p&gt;

&lt;p&gt;\[ d\theta = \frac {ds}{r} \]&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;对这个式子做0到2π的积分的话，显然右边的分子变成了圆周长2πr，圆心角为\( \frac { 2πr }{r} = 2π \)。&lt;/p&gt;

&lt;p&gt;立体角与圆心角非常类似。立体角的ds的含义是球面上的面积微分(下文用dA表示)，而分母需要变成半径r的平方（1&lt;a href=&quot;https://en.wikipedia.org/wiki/Steradian&quot;&gt;球面度&lt;/a&gt;所对应的立体角所对应的球面表面积为\(r^{2}\) ）：&lt;/p&gt;

&lt;p&gt;\[ d\omega = \frac {dA}{r^{2}} \]&lt;/p&gt;

&lt;p&gt;因为球体表面积等于\( 4πr^{2} \)，所以上面的式子积分到整个球体的话，立体角等于4π。&lt;/p&gt;

&lt;p&gt;再换个角度分析。在宏观上看，立体角的定义是：&lt;/p&gt;

&lt;p&gt;\[ \Omega = \frac {A}{r^{2} } sr \]&lt;/p&gt;

&lt;p&gt;其中，sr是单位，叫做球面度；A是这个立体角所对应的球表面积，A被叫做&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Spherical_cap&quot;&gt;spherical cap&lt;/a&gt;&lt;/strong&gt;(球帽?)。&lt;/p&gt;

&lt;p&gt;spherical cap的几何表示如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/8.png&quot; alt=&quot;8.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;(from wiki)&lt;/p&gt;

&lt;p&gt;spherical cap面积等于\(2\pi rh \)，所以上式可变成：&lt;/p&gt;

&lt;p&gt;\[ \Omega = \frac {2\pi rh}{r^{2} } sr \]&lt;/p&gt;

&lt;p&gt;当h等于r时，得到：&lt;/p&gt;

&lt;p&gt;\[ \Omega = \frac {2\pi rr}{r^{2} } sr = 2\pi \ sr \]&lt;/p&gt;

&lt;p&gt;此时得到的是半球的立体角，那么就可以知道整个球的立体角为\(4\pi  \)，和上述结论一致。&lt;/p&gt;

&lt;h2&gt;立体角(Solid Angle)转换到球形角(Sphere Angle)&lt;/h2&gt;

&lt;p&gt;Spherical Coordinates坐标系下的单位球，可用2个&lt;strong&gt;弧度变量&lt;/strong&gt;来定位球面上一个点：\(\theta 和 \phi \)。和三维坐标系的对应关系如下：&lt;/p&gt;

&lt;p&gt;\[ x = sin\theta cos\phi \]&lt;/p&gt;

&lt;p&gt;\[ y = sin\theta sin\phi \]&lt;/p&gt;

&lt;p&gt;\[ x = cos\theta \]&lt;/p&gt;

&lt;p&gt;简单验证下。把上面3个式子代入单位球公式:\(x^{2}+y^{2}+z^{2} = 1\)，可得：&lt;/p&gt;

&lt;p&gt;\[ (sin\theta cos\phi)^{2} + (sin\theta sin\phi )^{2} + cos^{2}\theta = 1 \]&lt;/p&gt;

&lt;p&gt;\[ sin^{2}\theta (cos^{2}\phi  + sin^{2}\phi ) + cos^{2}\theta = 1 \]&lt;/p&gt;

&lt;p&gt;\[ sin^{2}\theta  + cos^{2}\theta = 1 \]&lt;/p&gt;

&lt;p&gt;那么，立体角\(\omega \)和\(\theta 、 \phi \)是什么关系呢？先给出答案：&lt;/p&gt;

&lt;p&gt;\[ d\omega = sin\theta d\theta d\phi \]&lt;/p&gt;

&lt;p&gt;似乎有点莫名其妙，这里我详细解释吧。首先先搞懂\(  d\theta 和  d\phi \)的几何意义。&lt;/p&gt;

&lt;p&gt;弧度变量的单位是&lt;strong&gt;弧度&lt;/strong&gt;，1弧度的定义是：弧长s等于半径r的弧对应的圆心角为1弧度。因此可以知道整个圆的弧度为周长\(2\pi r\)除以半径\(r\)等于\(2\pi \)。&lt;/p&gt;

&lt;p&gt;因此，如果已知弧度和半径，就可以求出弧长s，那么上面的\(\theta 、 \phi \)对应的弧长就是：&lt;/p&gt;

&lt;p&gt;\[ s_{ \theta } = r_{ \theta }\theta  \]&lt;/p&gt;

&lt;p&gt;\[ s_{ \phi } = r_{ \phi }\phi  \]&lt;/p&gt;

&lt;p&gt;微分形式：&lt;/p&gt;

&lt;p&gt;\[ ds_{ \theta } = r_{ \theta } d\theta  \]&lt;/p&gt;

&lt;p&gt;\[ ds_{ \phi } = r_{ \phi } d\phi  \]&lt;/p&gt;

&lt;p&gt;\( r_{ \theta }、r_{ \phi }\)的值并不是相等的，需要接着分析。&lt;/p&gt;

&lt;p&gt;在球坐标系下，\(\theta 、 \phi \)指的是这2个角：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/9.png&quot; alt=&quot;9.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;\(\theta\)是目标点p与z轴的夹角；而\(\phi\)是目标点p在xy平面上的投影与y轴的夹角。&lt;/p&gt;

&lt;p&gt;从图可知，\( r_{ \theta } \)与圆的半径r相等；而\( r_{ \phi } \)是小于等于r的(注意看上面的小圆)，且有：&lt;/p&gt;

&lt;p&gt;\[ sin\theta = \frac { r_{ \phi } } {r_{ \theta } }\]&lt;/p&gt;

&lt;p&gt;当球是单位球时，球的半径为1，所以有：&lt;/p&gt;

&lt;p&gt;\[ r_{ \theta } = 1\]&lt;/p&gt;

&lt;p&gt;\[ r_{ \phi } = sin\theta  r_{ \theta } = sin\theta \]&lt;/p&gt;

&lt;p&gt;又因为在微观下，立体角对应的曲面(或者叫球帽)面积可以当做一个小矩形看，这个小矩形dA的面积等于2个弧长\( ds_{ \theta } \)和\( ds_{ \phi } \)的积：&lt;/p&gt;

&lt;p&gt;\[ dA = ds_{ \theta }ds_{ \phi } = r_{ \theta } r_{ \phi }d\theta d\phi = sin\theta d\theta d\phi \]&lt;/p&gt;

&lt;p&gt;再因为立体角的微分其实也就是这个小矩形的面积，那么就有：&lt;/p&gt;

&lt;p&gt;\[ d\omega = dA = sin\theta d\theta d\phi \]&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/solid-angle/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/solid-angle/</guid>
      </item>
    
      <item>
        <title>直线与平面相交算法</title>
        <description>&lt;!--more--&gt;

&lt;h1&gt;平面的方程&lt;/h1&gt;

&lt;p&gt;设位于平面P上的某个点为\( \vec p_{0} \)(已知量)，平面P的法向量为\( \vec n \)(已知量)，该平面P的任意一点设为\( \vec p \)，则平面P的方程为：&lt;/p&gt;

&lt;p&gt;\[ (\vec p -  \vec p_{0} )\cdot \vec n = 0 \]&lt;/p&gt;

&lt;p&gt;使得这个方程成立的原理是，位于平面P上的任意一个向量\( (\vec p -  \vec p_{0} ) \)，必然和平面P的法向量垂直，互相垂直的向量的点积必然为0。&lt;/p&gt;

&lt;h1&gt;直线的方程&lt;/h1&gt;

&lt;p&gt;设位于直线上的某个点为\( \vec o \)(已知量)，直线的方向向量为\( \vec d \)(已知量)，直线上的任意一点为\( \vec p \)，则直线的方程为:&lt;/p&gt;

&lt;p&gt;\[ \vec p = \vec o + t\vec d, t\in R \]&lt;/p&gt;

&lt;p&gt;注意，式子右边的t是一个Scalar（一个实数），这个t决定了\(\vec p\)的位置。&lt;/p&gt;

&lt;h1&gt;直线与平面的相交&lt;/h1&gt;

&lt;p&gt;直线与平面的相交，必然是一个点。将上面的直线方程代入平面方程，得到：&lt;/p&gt;

&lt;p&gt;\[ ( \vec o + t\vec d -  \vec p_{0} )\cdot \vec n = 0  \]&lt;/p&gt;

&lt;p&gt;其中，\( \vec o 、\vec d 、\vec p_{0} 、\vec n \)都是已知量，只有t是未知量，&lt;strong&gt;所以求直线与平面的相交点等同于求t的值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;化简这个式子，得到：&lt;/p&gt;

&lt;p&gt;\[ t\vec d \cdot \vec n + (\vec o -  \vec p_{0} )\cdot \vec n = 0  \]&lt;/p&gt;

&lt;p&gt;\[ t\vec d \cdot \vec n = ( \vec p_{0} - \vec o )\cdot \vec n  \]&lt;/p&gt;

&lt;p&gt;\[ t = \frac { ( \vec p_{0} - \vec o )\cdot \vec n }{ \vec d \cdot \vec n }  \]&lt;/p&gt;

&lt;h1&gt;实例 (基于三维坐标系)&lt;/h1&gt;

&lt;p&gt;本小节的实例是结合实际需求设计的。在3D图形学中，直线与平面相交算法一般是应用到了光线与包围盒的相交判定问题中。&lt;/p&gt;

&lt;p&gt;那么下面就设直线为任意直线(光线都是任意方向的)，而平面设为某几种特定的特殊平面，来展示上面的算法的流程。&lt;/p&gt;

&lt;h2&gt;1. 法向量为\( (1,0,0) \)并且经过点\( (x_{0},0,0) \)的平面&lt;/h2&gt;

&lt;p&gt;根据上面的定义，可以知道：&lt;/p&gt;

&lt;p&gt;\[ \vec n =  (1,0,0)  \]&lt;/p&gt;

&lt;p&gt;\[ \vec p_{0} =  (x_{0},0,0)  \]&lt;/p&gt;

&lt;p&gt;代入上面的方程，得到：&lt;/p&gt;

&lt;p&gt;\[ t = \frac { ( (x_{0},0,0) - \vec o )\cdot (1,0,0) }{ \vec d \cdot (1,0,0) }  \]&lt;/p&gt;

&lt;p&gt;化简得到：&lt;/p&gt;

&lt;p&gt;\[ t = \frac { x_{0} -  o_{x} }{ d_{x} }  \]&lt;/p&gt;

&lt;h2&gt;2. 法向量为\( (0,1,0) \)并且经过点\( (0,y_{0},0) \)的平面&lt;/h2&gt;

&lt;p&gt;同上，可以得到：&lt;/p&gt;

&lt;p&gt;\[ t = \frac { y_{0} -  o_{y} }{ d_{y} }  \]&lt;/p&gt;

&lt;h2&gt;3. 法向量为\( (0,0,1) \)并且经过点\( (0,0,z_{0}) \)的平面&lt;/h2&gt;

&lt;p&gt;同上，可以得到：&lt;/p&gt;

&lt;p&gt;\[ t = \frac { z_{0} -  o_{z} }{ d_{z} }  \]&lt;/p&gt;

&lt;h1&gt;应用，判定直线与BBox是否相交&lt;/h1&gt;

&lt;p&gt;BBox是一个分别和x、y、z轴平行的长方体。在判定直线与BBox是否相交的问题中，要把BBox想象成6个无限大的平面，每个平面称为一个slab。然后就可以应用上面的公式，计算直线与6个slab的相交点(即，算出t值)，根据t值，就可以知道直线是不是和BBox相交了。&lt;/p&gt;

&lt;h2&gt;2维的情况&lt;/h2&gt;

&lt;p&gt;以此图为例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/1.png&quot; alt=&quot;1.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;图中的红色区域是一个BBox，一般定义一个BBox只需要记录它的2个对角顶点的坐标值即可。图中BBox的左下顶点坐标为(3,1)，右上顶点坐标为(6,3)；&lt;/p&gt;

&lt;p&gt;图中绿色直线的方程为 y = x - 1。&lt;/p&gt;

&lt;p&gt;BBox的4个slab分别为:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;法向量为\( (1,0) \)并且经过点\( (3,0) \)的直线a&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;法向量为\( (1,0) \)并且经过点\( (6,0) \)的直线b&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;法向量为\( (0,1) \)并且经过点\( (0,1) \)的直线c&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;法向量为\( (0,1) \)并且经过点\( (0,3) \)的直线d&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;绿色直线的o和d分别为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;o = (1,0)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;d = (1,1)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据上面的交点公式，可以知道绿色直线与4个slab的相交点A、B、C、D，它们的t值分别为：&lt;/p&gt;

&lt;p&gt;\[ t_{A} = \frac { x_{0} -  o_{x} }{ d_{x} }  = \frac { 3 - 1 }{ 1 } = 2  \]&lt;/p&gt;

&lt;p&gt;\[ t_{B} = \frac { x_{1} -  o_{x} }{ d_{x} }  = \frac { 6 - 1 }{ 1 } = 5  \]&lt;/p&gt;

&lt;p&gt;\[ t_{C} = \frac { y_{0} -  o_{y} }{ d_{y} }  = \frac { 1 - 0 }{ 1 } = 1  \]&lt;/p&gt;

&lt;p&gt;\[ t_{D} = \frac { y_{1} -  o_{y} }{ d_{y} }  = \frac { 3 - 0 }{ 1 } = 3  \]&lt;/p&gt;

&lt;p&gt;得到这4个t值之后，怎么知道相交还是不相交呢？这需要分析一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;\( t_{A} \)、\( t_{B} \)的意义是：对于在BBox内的任意一个点P，它的分量x关于直线的t值，必然在\( [ t_{A} ,  t_{B} ] \)之间；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( t_{C} \)、\( t_{D} \)的意义是：对于在BBox内的任意一个点P，它的分量y关于直线的t值，必然在\( [ t_{C} ,  t_{D} ] \)之间；&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;结合例子来说就是：若BBox内存在一个点P，它的x、y分量的t值分别满足\( 2 \leq t_{x} \leq 5 \)、\( 1 \leq t_{y} \leq 3 \)时，这个BBox就必然与直线y = x - 1相交。&lt;/p&gt;

&lt;p&gt;但是这个方案并不是最佳方案。&lt;/p&gt;

&lt;p&gt;接着分析下。&lt;/p&gt;

&lt;p&gt;因为我们给直线定义了o和d，所以直线的另一个表达式是 \(\vec o + t\vec d\)。根据这个表达式，\( t_{A} \)、\( t_{B} \) 和  \( t_{C} \)、\( t_{D} \)有了新的意义：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;\( t_{A} \)、\( t_{B} \)限定了一条在直线上的有限长度线段\( \overline {AB} \)，起点是\(\vec o + t_{A}\vec d \)，终点是\(\vec o + t_{B}\vec d \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( t_{C} \)、\( t_{D} \)限定了一条在直线上的有限长度线段\( \overline {CD} \)，起点是\(\vec o + t_{C}\vec d \)，终点是\(\vec o + t_{D}\vec d \)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;于是，判定直线与BBox是否相交的充分必要条件呼之欲出：&lt;strong&gt;s1和s2两个线段存在重叠&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/2.png&quot; alt=&quot;2.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;图中的黑色线段即为\( \overline {AB} \)、\( \overline {CD} \)的重叠部分。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.7/3.png&quot; alt=&quot;3.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;对于直线y = x + 1来说，就不存在重叠部分了。&lt;/p&gt;

&lt;h2&gt;3维的情况&lt;/h2&gt;

&lt;p&gt;理解了2维的情况，3维（甚至更高维）的情况类推就可以了。完整的算法流程是：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BBox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IsIntersectWithRay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Ray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//x y z&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invRayDir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tNear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pMin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invRayDir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tFar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pMax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;invRayDir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tNear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tFar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tNear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tFar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tNear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tFar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        <pubDate>Fri, 01 Jul 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/line-plane-intersection/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/line-plane-intersection/</guid>
      </item>
    
      <item>
        <title>Global Gamejam 2016 和 IndieAce Gamejam 2016 作品</title>
        <description>&lt;!--more--&gt;

&lt;h1&gt;Global Gamejam&lt;/h1&gt;

&lt;p&gt;本次主题，一个词：&lt;/p&gt;

&lt;h2&gt;&lt;strong&gt;Ritual&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;我们根据题目做出来的游戏：《阴阳合》&lt;/p&gt;

&lt;p&gt;游戏截图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.6/2.png&quot; alt=&quot;2.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;试玩地址：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://yinyang.qiujiawei.com/&quot;&gt;http://yinyang.qiujiawei.com/&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;IndieACE Gamejam&lt;/h1&gt;

&lt;p&gt;本次主题，一幅画：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.6/1.jpg&quot; alt=&quot;1.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;我们根据题目做出来的游戏：《人，造人》&lt;/p&gt;

&lt;p&gt;游戏截图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/2016.6/3.jpg&quot; alt=&quot;3.jpg&quot;&gt;&lt;/p&gt;

&lt;p&gt;试玩地址：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://voyagingmk.github.io/IndieACEGamejam2016/&quot;&gt;https://voyagingmk.github.io/IndieACEGamejam2016/&lt;/a&gt;&lt;/p&gt;

&lt;h1&gt;后记&lt;/h1&gt;

&lt;p&gt;GGJ一般是年初的时候举办，IndieACE的GJ一般是年中的时候举办。目前国内搞得好的、一般人能接触到的只有这2个GJ吧。&lt;/p&gt;
</description>
        <pubDate>Mon, 27 Jun 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/gamejam/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/gamejam/</guid>
      </item>
    
      <item>
        <title>学习增强学习</title>
        <description>&lt;!--more--&gt;

&lt;h1&gt;马尔可夫决策过程&lt;/h1&gt;

&lt;p&gt;马尔可夫决策过程(&lt;a href=&quot;https://en.wikipedia.org/wiki/Markov_decision_process&quot;&gt;Markov decision process&lt;/a&gt;，下文简称MDP)，可用来处理一些最优化问题，譬如非常出名的&lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;动态规划&lt;/a&gt;问题，以及本文的核心——&lt;a href=&quot;https://en.wikipedia.org/wiki/Reinforcement_learning&quot;&gt;增强学习&lt;/a&gt;问题。&lt;/p&gt;

&lt;h2&gt;MDP的定义&lt;/h2&gt;

&lt;p&gt;MDP包含5个东西：\(S、A、P、R、\gamma \)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;S(State)，指的是目标系统的所有状态的集合&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A(Action)，状态之间的转换行为。可以把状态S想象成一堆节点，而A就是各个节点之间的有向连线集合&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;P(probability)，\(P_{a}(s,s&amp;#39;)\)，某状态\(s\)通过某行为a进入另一个状态\(s&amp;#39;\)的概率&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;R(Rewawrd)，\(R_{a}(s,s&amp;#39;)\)，某状态\(s\)通过某行为a进入另一个状态\(s&amp;#39;\)能获得的奖励值&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\(\gamma \) (Discount Factor)，用于奖励值计算的一个系数，表示非立即奖励(future rewards)相对于立即奖励(present rewards)的重要程度，当为1时表示同等重要，小于1时表示非立即奖励要打个折，等于0时表示只考虑立即奖励。因此有\( 0 \leq \gamma \leq 1 \)。注意，\(\gamma \) 是一个常数&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为一般来说，状态之间是有时间先后顺序的（拓扑结构），所以每个状态s有它自己的固有属性t，表示这个s处于时间轴上的位置。&lt;/p&gt;

&lt;p&gt;在MDP的定义中没有指出S、A是不是有限集合(finite)，但在实际应用MDP时，必然是有限的。&lt;/p&gt;

&lt;p&gt;MDP的直观表示是一个有向图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/2/21/Markov_Decision_Process_example.png&quot; alt=&quot;1.png&quot;&gt;
(from wiki)&lt;/p&gt;

&lt;h2&gt;MDP的核心问题&lt;/h2&gt;

&lt;p&gt;MDP的核心核心是找出一个最理想的策略函数\(\pi ^{*} (s)\)给做决策的人，这个\(\pi ^{*}(s)\)决定了当处于状态s时，应采取哪个行为a，即\( a_{t} = \pi ^{*} (s_{t})\)。&lt;/p&gt;

&lt;p&gt;这个问题的解决方案是：求出一个叫做&lt;strong&gt;折算累积奖励(discounted cumulative reward)&lt;/strong&gt;多项式的最大值，此时的\(\pi (s)\)就是最优解\(\pi ^{*}(s)\)。公式化表示就是：&lt;/p&gt;

&lt;p&gt;\[ r_{t} =  R_{ a_{t} }(s_{t}, s_{t+1} ) \]&lt;/p&gt;

&lt;p&gt;\[ V ^{\pi } (s_{t}) = r_{t} + \gamma r_{t+1} + \gamma ^{2} r_{t+2} + \cdots  =  \sum _{i=0}^{\infty }\gamma ^{i}r_{t+i} \]&lt;/p&gt;

&lt;p&gt;\[\pi ^{*} = argmax  V ^{\pi } (s) \]&lt;/p&gt;

&lt;p&gt;获得\(\pi ^{*} (s)\)后，就可以应用了，即对任意一个\(s_{t}\)，都可以算出最理想的行为\( a^{*}_{t} = \pi ^{*} (s_{t})\)。&lt;/p&gt;

&lt;p&gt;注意：\( V ^{\pi } (s_{t}) \)的实现不是唯一的，还有其他各种各样的公式可以选择，严格来说并不是MDP定义的一部分。&lt;/p&gt;

&lt;h2&gt;Q函数&lt;/h2&gt;

&lt;p&gt;评估函数Q的公式定义：&lt;/p&gt;

&lt;p&gt;\[ Q(s,a) = r_{immediate}(s,a) + \gamma V^{ \pi ^{*} }(s&amp;#39;) \]&lt;/p&gt;

&lt;p&gt;用文字解释：Q的值为从状态s执行动作a所获得的立即奖励再加上后续遵循最优策略时的V值，V用\(\gamma \)折算。&lt;/p&gt;

&lt;p&gt;并且有：&lt;/p&gt;

&lt;p&gt;\[\pi ^{*} = argmax Q(s,a)  \]&lt;/p&gt;

&lt;p&gt;V函数和Q函数的关系：&lt;/p&gt;

&lt;p&gt;\[ V^{ \pi ^{*} } = \max _{a&amp;#39;}Q(s,a&amp;#39;) \]&lt;/p&gt;

&lt;p&gt;用这个式子重写Q的定义式：&lt;/p&gt;

&lt;p&gt;\[ Q(s,a) = r_{immediate}(s,a) + \gamma \max _{a&amp;#39;}Q(s&amp;#39;,a&amp;#39;) \]&lt;/p&gt;

&lt;h2&gt;确定性MDP系统的基于Q函数的增强学习算法&lt;/h2&gt;

&lt;p&gt;在增强学习中，要学习的函数是Q函数而不是\( V^{ \pi ^{*} } \)函数。这是因为后者是关于s的一元函数，计算过程要求知道每个状态\(s_{t}\)的最佳\(a_{t}\)，否则就算不出\(r_{t}\)了；而Q函数是关于s、a的二元函数，不需要知道最佳\(a_{t}\)，而仅仅需要知道\( r_{immediate}(s,a) \)的值（右边的\(  \gamma \max _{a&amp;#39;}Q(s&amp;#39;,a&amp;#39;) \)是递归式，熟悉动态规划的童鞋就知道Q可以计算出来的，实际上Q就是一个&lt;a href=&quot;https://en.wikipedia.org/wiki/Bellman_equation&quot;&gt;Bellman方程&lt;/a&gt;）；当算出所有Q(s,a)的值后，根据\( a_{t}  = argmax Q(s_{t},a) \)，就可以知道\( \pi ^{*} \)。&lt;/p&gt;

&lt;p&gt;Q学习算法的流程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;建一个二维表Q(s,a)，并把所有表项Q(s,a)初始化成0&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;重复迭代：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;根据当前状态s，选一个动作a并执行&lt;/li&gt;
&lt;li&gt;得到立即奖励r：\( r = R_{ a }(s, s&amp;#39;)  \)&lt;/li&gt;
&lt;li&gt;更新表项：\( Q(s,a) = r + \gamma \max _{a&amp;#39;}Q(s&amp;#39;, a&amp;#39;)  \) 【向后传播(back propagation)】&lt;/li&gt;
&lt;li&gt;进入新状态s&amp;#39;：\(  s = s&amp;#39; \)&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 11 Jun 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/reinforcement-learning/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/reinforcement-learning/</guid>
      </item>
    
      <item>
        <title>线性代数之主成分分析(PCA)算法</title>
        <description>&lt;p&gt;PCA(Principal Component Analysis)的主要应用场景是：在大数据集中找出关键的信息并剔除冗余的信息。根据这个特性，PCA也可以用来做信息压缩(有损)、特征提取。不过在本文中，只会对PCA的数学原理进行阐述。&lt;/p&gt;

&lt;p&gt;另外，PCA可以说是Machine Learning领域的自编码机(AutoEncoder,AE)的基础。主要区别在于，PCA是线性算法，而AE则不一定。所以在学习AutoEncoder之前，有必要先将PCA搞清楚。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1&gt;Part I&lt;/h1&gt;

&lt;p&gt;设向量\( \vec x \)表示对某个特征的n次采样(测量), 那么如果有m个不同的特征，就组成了一个\(m\times n \)的矩阵\( X \)：&lt;/p&gt;

&lt;p&gt;\[ X =  \left[ \begin{matrix} \vec x_{1}\\   \vec x_{2}\\   \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;然后问题来了：每个特征之间是否是&lt;strong&gt;相互独立(independant)&lt;/strong&gt;的？如果是，那么说明这m个特征是良好的，可以直接拿去应用到任务中(譬如基于这些特征做一个分类器)；如果不是，那么就说明有特征是多余的，譬如\(  \vec x_{a} \)、\(  \vec x_{b} \)分别用米和英尺记录了同一个特征，虽然数值不一样，然而并没有什么卵用。&lt;/p&gt;

&lt;p&gt;量化特征与特征之间的关系的最好办法是用&lt;strong&gt;方差&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Variance&quot;&gt;Variance&lt;/a&gt;)和&lt;strong&gt;协方差&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Covariance&quot;&gt;Covariance&lt;/a&gt;)，这2者又共同涉及到了更基础的概念&lt;strong&gt;数学期望&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Expected_value&quot;&gt;Expected Value&lt;/a&gt;)和&lt;strong&gt;均值&lt;/strong&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Mean&quot;&gt;Mean&lt;/a&gt;)。先简单过一遍这4个东西的公式。&lt;/p&gt;

&lt;h3&gt;数学期望和均值&lt;/h3&gt;

&lt;p&gt;数学期望公式：&lt;/p&gt;

&lt;p&gt;\[ E[\vec x] = \sum _{i=1}^{n}x_{i}p_{i} \]&lt;/p&gt;

&lt;p&gt;当每个\( x_{i} \)的出现概率相等时(均匀分布)，有\( p_{i} = \frac {1}{n} \)，所以上式可简化成:&lt;/p&gt;

&lt;p&gt;\[ E[\vec x] = \frac {1}{n}\sum _{i=1}^{n }x_{i} \]&lt;/p&gt;

&lt;p&gt;上式其实也就是均值\( \overline {x} \)的定义，所以当\(x_{i}\)均匀分布时，有：&lt;/p&gt;

&lt;p&gt;\[  E[\vec x] =  \overline {x} \]&lt;/p&gt;

&lt;p&gt;有时候也用\( \mu \)来指代Mean。&lt;/p&gt;

&lt;h3&gt;方差和协方差&lt;/h3&gt;

&lt;p&gt;方差:&lt;/p&gt;

&lt;p&gt;\[ Var(\vec x) = E[ (\vec x - E[\vec x])^{2 } ] = E[ (\vec x - E[\vec x])(\vec x -  E[\vec x]) ]  \]&lt;/p&gt;

&lt;p&gt;协方差:&lt;/p&gt;

&lt;p&gt;\[ Cov(\vec x, \vec y) = E[ (\vec x -  E[\vec x])(\vec y -  E[\vec y]) ] \]&lt;/p&gt;

&lt;p&gt;可以发现方差是协方差的特殊情况:&lt;/p&gt;

&lt;p&gt;\[ Var(\vec x) = Cov(\vec x, \vec x) \]&lt;/p&gt;

&lt;h3&gt;协方差矩阵&lt;/h3&gt;

&lt;p&gt;在&lt;a href=&quot;http://daobiao.win:4000/linear-algebra-7/&quot;&gt;线性代数之各种各样的矩阵&lt;/a&gt;最后面已经提到了协方差矩阵(Covariance matrix):&lt;/p&gt;

&lt;p&gt;\[ C =  \left[ \begin{matrix} E[(\vec x_{1} - \mu_{1})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{1} - \mu_{1})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{1} - \mu_{1})(\vec x_{m} - \mu_{m})]\\            E[(\vec x_{2} - \mu_{2})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{2} - \mu_{2})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{2} - \mu_{2})(\vec x_{m} - \mu_{m})]\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\         E[(\vec x_{m} - \mu_{m})(\vec x_{1} - \mu_{1})]&amp;amp;  E[(\vec x_{m} - \mu_{m})(\vec x_{2} - \mu_{2})]&amp;amp;  \cdots &amp;amp; E[(\vec x_{m} - \mu_{m})(\vec x_{m} - \mu_{m})]\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;h3&gt;当Mean等于0时的情况&lt;/h3&gt;

&lt;p&gt;当Mean等于0时，上面的协方差矩阵变成：&lt;/p&gt;

&lt;p&gt;\[ C =  \left[ \begin{matrix} E[\vec x_{1}\vec x_{1}]&amp;amp;  E[\vec x_{1} \vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{1}\vec x_{m}]\\            E[\vec x_{2}\vec x_{1}]&amp;amp;  E[\vec x_{2}\vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{2}\vec x_{m}]\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\         E[\vec x_{m}\vec x_{1}]&amp;amp;  E[\vec x_{m}\vec x_{2}]&amp;amp;  \cdots &amp;amp; E[\vec x_{m}\vec x_{m}]\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;再假设\( \vec x \)每个分量的取值是均匀分布的，那么根据上面的定义，有：&lt;/p&gt;

&lt;p&gt;\[E[\vec x_{a}\vec x_{b}] = \frac {1}{n}\sum _{i=1}^{n} \vec x_{ai}\vec x_{bi} , 1 \leq a\leq m, 1 \leq b\leq m  \]&lt;/p&gt;

&lt;p&gt;代入上式，得到：&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n} \left[ \begin{matrix} \sum _{i=1}^{n} \vec x_{1}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{1}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{1}\vec x_{m}\\            \sum _{i=1}^{n} \vec x_{2}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{2}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{2}\vec x_{m}\\   \vdots &amp;amp; \vdots &amp;amp;  \ddots &amp;amp; \vdots \\        \sum _{i=1}^{n} \vec x_{m}\vec x_{1}&amp;amp;  \sum _{i=1}^{n} \vec x_{m}\vec x_{2}&amp;amp;  \cdots &amp;amp; \sum _{i=1}^{n} \vec x_{m}\vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;再设一个矩阵X：&lt;/p&gt;

&lt;p&gt;\[ X =  \left[ \begin{matrix} \vec x_{1}\\  \vec x_{2}\\  \vdots \\  \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp;amp; \vec x_{2}&amp;amp; \cdots &amp;amp; \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;于是有：&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n}XX^{T} \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;总结下&lt;/strong&gt;，对符合均匀分布的、且均值等于0的\(\vec x_{i, 1\leq i \leq m}\)，它的协方差矩阵如下：&lt;/p&gt;

&lt;p&gt;\[ X^{T} =  \left[ \begin{matrix} \vec x_{1}&amp;amp; \vec x_{2}&amp;amp; \cdots &amp;amp; \vec x_{m}\\ \end{matrix} \right]  \]&lt;/p&gt;

&lt;p&gt;\[ C = \frac {1}{n}XX^{T} \]&lt;/p&gt;

&lt;p&gt;为了下文能继续推导，需要把C记为\( C_{x} \)。&lt;/p&gt;

&lt;h1&gt;PART II&lt;/h1&gt;

&lt;p&gt;根据上面得到的协方差矩阵的公式，可以知道：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;\( C_{x} \)是一个对称方阵&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( C_{x} \)的对角线上的元素分别代表了对某个特征的n次测量的方差&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( C_{x} \)的非对角线上的元素代表了任意2个特征之间的协方差&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;开始进入到PCA的环节。PCA的目标是提炼出\( C_{x} \)的关键信息并剔除冗余信息，这个过程用线性代数表示就是：&lt;/p&gt;

&lt;p&gt;\[ Y  = PX  \]&lt;/p&gt;

&lt;p&gt;这里面P就是我们需要的目标矩阵。而关于矩阵Y的协方差矩阵\( C_{y} \)的特性是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;\( C_{y} \)的对角线上的元素(方差)尽可能大（增大信号）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( C_{y} \)的非对角线上的元素(协方差)应该等于零，因此\( C_{y} \)还是一个对角线矩阵&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\( C_{x} \)、\( C_{y} \)、P的关系是：&lt;/p&gt;

&lt;p&gt;\[  C_{y} =  \frac {1}{n}YY^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = \frac {1}{n}(PX)(PX)^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = \frac {1}{n}PXX^{T}P^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = P(\frac {1}{n}XX^{T})P^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = PC_{x}P^{T} \]&lt;/p&gt;

&lt;p&gt;即：&lt;/p&gt;

&lt;p&gt;\[   C_{y} = PC_{x}P^{T} \]&lt;/p&gt;

&lt;p&gt;PCA的求解方法多种多样，下面展示最经典的解法——特征值分解。&lt;/p&gt;

&lt;h2&gt;基于特征值分解的PCA&lt;/h2&gt;

&lt;p&gt;在&lt;a href=&quot;http://127.0.0.1:4000/linear-algebra-6/&quot;&gt;矩阵的特征值、特征向量、特征矩阵、迹、特征值分解&lt;/a&gt;一文中提到了对称方阵的特征值分解公式：&lt;/p&gt;

&lt;p&gt;\[ A =  S\Lambda S^{-1} = S\Lambda S^{T} \]&lt;/p&gt;

&lt;p&gt;矩阵\( \Lambda \)是对角矩阵，对角线上的元素为特征值；矩阵S是一个行向量为特征向量的矩阵，\( S^{-1} = S^{T} \)。&lt;/p&gt;

&lt;p&gt;有了这个公式后，立即可以知道对称矩阵\( C_{x} \)的特征值分解(对角化)为：&lt;/p&gt;

&lt;p&gt;\[ C_{x} =  S\Lambda S^{T} \]&lt;/p&gt;

&lt;p&gt;将其代入上面的公式，有:&lt;/p&gt;

&lt;p&gt;\[C_{y} = PS\Lambda S^{T}P^{T}   \]&lt;/p&gt;

&lt;p&gt;这里可以大胆做个假设：\( P \equiv  S^{T} \)。又因为 \( S^{-1} = S^{T} \)， 则有:&lt;/p&gt;

&lt;p&gt;\[ P^{-1} =  (S^{T})^{-1} = (S^{-1})^{-1} = S = P^{T} \]&lt;/p&gt;

&lt;p&gt;所以：&lt;/p&gt;

&lt;p&gt;\[ C_{y} = PS\Lambda S^{T}P^{T} = PP^{T} \Lambda PP^{T} = PP^{-1} \Lambda PP^{-1} = \Lambda   \]&lt;/p&gt;

&lt;p&gt;总结：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当\( P \)(主成分矩阵)是\( C_{x} \)的特征向量矩阵\( S \)时，\( C_{y} \)是特征值矩阵\( \Lambda\)&lt;/strong&gt;。&lt;/p&gt;

&lt;h2&gt;基于奇异值分解的PCA&lt;/h2&gt;

&lt;p&gt;关于SVD的讨论我都会追加&lt;a href=&quot;http://127.0.0.1:4000/linear-algebra-9/&quot;&gt;线性代数之奇异值(SVD)分解&lt;/a&gt;一文中，在这里不展开对SVD的详细讨论。&lt;/p&gt;

&lt;p&gt;SVD分解公式：&lt;/p&gt;

&lt;p&gt;\[ M = UΣV^{*} \]&lt;/p&gt;

&lt;p&gt;（其中各个矩阵的含义就不赘述了）&lt;/p&gt;

&lt;p&gt;和PCA最相关的SVD其中一条性质：&lt;/p&gt;

&lt;p&gt;\[ M^{*}M = V\Sigma ^{*}U^{*}U\Sigma V^{*} = V\Sigma ^{*}(U^{*}U)\Sigma V^{*} = V\Sigma ^{*}\Sigma V^{*} = V(\Sigma  ^{*}\Sigma )V^{-1}  \]&lt;/p&gt;

&lt;p&gt;当元素是实数时，有：&lt;/p&gt;

&lt;p&gt;\[ M^{T}M = V(\Sigma  ^{T}\Sigma )V^{-1}  \]&lt;/p&gt;

&lt;p&gt;注意，这个公式包含了一个事实：\( M^{T}M \)的特征值分解等于\( V(\Sigma  ^{T}\Sigma )V^{-1}  \)。&lt;/p&gt;

&lt;p&gt;现在，假设有一个矩阵Y，Y满足：&lt;/p&gt;

&lt;p&gt;\[ Y = \frac { 1 }{ \sqrt {n} }X^{T} \]&lt;/p&gt;

&lt;p&gt;那么有：&lt;/p&gt;

&lt;p&gt;\[ Y^{T}Y = (\frac { 1 }{ \sqrt {n} }X^{T})^{T}\frac { 1 }{ \sqrt {n} }X^{T} \]&lt;/p&gt;

&lt;p&gt;\[ = \frac { 1 }{ \sqrt {n} }\frac { 1 }{ \sqrt {n} } (X^{T})^{T}X^{T} \]&lt;/p&gt;

&lt;p&gt;\[ = \frac { 1 }{ n }XX^{T} \]&lt;/p&gt;

&lt;p&gt;\[ = C_{x} \]&lt;/p&gt;

&lt;p&gt;而Y的SVD分解为：&lt;/p&gt;

&lt;p&gt;\[ Y = UΣV^{*} \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ Y^{T}Y = V(\Sigma  ^{T}\Sigma )V^{-1} = C_{x} \] &lt;/p&gt;

&lt;p&gt;这时候事情又和特征值分解联系上了，在上一小节我们已经知道&lt;strong&gt;当\( P \)是\( C_{x} \)的特征向量矩阵\( S \)时，\( C_{y} \)是特征值矩阵\( \Lambda\)&lt;/strong&gt;，而在这个式子中，\(V\)就是\( C_{x} \)的特征向量矩阵！&lt;/p&gt;

&lt;p&gt;整理一下这个解法的思路：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;先设\( Y = \frac { 1 }{ \sqrt {n} }X^{T} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对矩阵Y做SVD分解，得到矩阵V，V就是关于X的主成分矩阵&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;参考资料&lt;/h1&gt;

&lt;p&gt;A Tutorial on Principal Component Analysis, Jonathon Shlens&lt;/p&gt;
</description>
        <pubDate>Mon, 30 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-17/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-17/</guid>
      </item>
    
      <item>
        <title>线性代数之伪逆矩阵(pseudoinverse matrix)</title>
        <description>&lt;p&gt;众所周知只有方阵才有逆矩阵，非方阵没有逆矩阵。这个不和谐的问题已在20世纪初被数学家&lt;a href=&quot;https://en.wikipedia.org/wiki/E._H._Moore&quot;&gt;E. H. Moore&lt;/a&gt;等人解决掉了，因为他们发明了&lt;strong&gt;一般化的逆矩阵(generalized inverse)&lt;/strong&gt;，也称为&lt;strong&gt;伪逆矩阵(Moore–Penrose pseudoinverse)&lt;/strong&gt;。&lt;/p&gt;

&lt;!--more--&gt;

&lt;h1&gt;定义&lt;/h1&gt;

&lt;p&gt;对于任意一个矩阵A，A的伪逆矩阵\(A ^{+} \)必然存在，且\(A ^{+} \)必然满足以下四个条件：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;\( AA ^{+}A = A \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( A ^{+}AA ^{+} = A ^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (AA ^{+})^{*} = AA ^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A ^{+}A)^{*} = A ^{+}A \)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这四个条件(性质)蕴含了一个事情：\(AA ^{+} \)必然是一个效果等同单位矩阵I、但又不是单位矩阵I的矩阵。&lt;/p&gt;

&lt;p&gt;伪逆矩阵\( A ^{+} \)的极限形式定义：&lt;/p&gt;

&lt;p&gt;\[ A^{+} = \lim _{\delta \searrow 0} ( A^{*}A + \delta I ) ^{-1}A^{*} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{\delta \searrow 0}A^{*} ( A^{*}A + \delta I ) ^{-1} \]&lt;/p&gt;

&lt;p&gt;伪逆矩阵更加常用的定义（基于SVD奇异值分解）：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;SVD公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ A = UΣV^{*} \]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;伪逆矩阵公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\[ A^{+} = VΣ^{+}U^{*} \]&lt;/p&gt;

&lt;p&gt;这个公式要注意的是中间的\(Σ^{+}\)的求法。因为\(Σ_{m\times n}\)是一个对角线矩阵，但又不一定是方阵，所以计算它的伪逆矩阵的步骤是特殊又简单的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;将对角线上的元素取倒数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;再将整个矩阵转置一次&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1&gt;性质&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;当A可逆时，A的伪逆矩阵等于A的逆矩阵&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;零矩阵的伪逆矩阵是它的转置矩阵&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{+})^{+} = A \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{+})^{T} = (A^{T})^{+} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( ( \overline {A} )^{+} =  \overline { A^{+} } \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (A^{*})^{+}  = (A^{+})^{*} \)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( (\alpha A)^{+}  = \alpha ^{-1}A^{+} \)，\(\alpha \)不等于0&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1&gt;应用：最小二乘估计&lt;/h1&gt;

&lt;p&gt;在我的&lt;a href=&quot;http://www.qiujiawei.com/linear-algebra-15/&quot;&gt;最小二乘估计(Least Squares Estimator)的公式的推导&lt;/a&gt;一文中，提到了一个小问题，&lt;strong&gt;当矩阵X不是方阵时，最小二乘估计公式必须为&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;不能进一步简化，除非X是有逆矩阵的方阵。&lt;/p&gt;

&lt;p&gt;这个事情也可以用伪逆矩阵来讨论一遍。&lt;/p&gt;

&lt;p&gt;先回到问题本源——最小二乘估计的本质是解决下面的方程：&lt;/p&gt;

&lt;p&gt;\[ \vec y = X\vec a + \vec e \]&lt;/p&gt;

&lt;p&gt;其中\(\vec y\)和\(X\)是已知量，\(\vec a\)和\(\vec e\)是要求的量，这可能有0到n个解，而我们的目标是想找使得\( ||\vec e||_{2}\)最小的\(\vec a\)。&lt;/p&gt;

&lt;p&gt;当我们求得理想的\(\vec a\)、\(\vec e\)后，可以让\(\vec e = \vec 0\)，并把\(\vec a\)、\(\vec e\)代入原方程，从而得到下面的等式：&lt;/p&gt;

&lt;p&gt;\[ \widehat {\vec y} = X\vec a \]&lt;/p&gt;

&lt;p&gt;求得的\( \widehat {\vec y} \)就是\( \vec y  \)的最佳近似。&lt;/p&gt;

&lt;p&gt;再换个角度想——如果我们一开始就默认方程\( \vec y = X\vec a  \)有解，那么这个解就是：&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}\vec y \]&lt;/p&gt;

&lt;p&gt;慢着！\(  X^{-1} \)可不一定存在的，因为X不一定是方阵，所以上面这个等式是错误的。怎么办？这时候伪逆矩阵就派上用场了：&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{+}\vec y \]&lt;/p&gt;

&lt;p&gt;因为伪逆矩阵对任意矩阵都存在，所以这个等式才是合理的。&lt;/p&gt;

&lt;h1&gt;参考资料&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse#Singular_value_decomposition_.28SVD.29&quot;&gt;Moore–Penrose pseudoinverse&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 29 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-16/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-16/</guid>
      </item>
    
      <item>
        <title>矩阵微分（一）</title>
        <description>&lt;h1&gt;基本认识&lt;/h1&gt;

&lt;h2&gt;3种标准导数(梯度)公式&lt;/h2&gt;

&lt;p&gt;1) 自变量是一个数值量(Scalar)时：&lt;/p&gt;

&lt;p&gt;\[ Df(x) = \lim _{t\to 0} \frac {f(x+t)-f(t)}{t} \]&lt;/p&gt;

&lt;p&gt;2) 自变量是一个向量(Vector)时：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {w}}f(\textbf {x}) = \lim _{t\to 0} \frac {f(\textbf {x} + t\textbf {w}) - f(t)}{t} \]&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;(w的维数和x一致)&lt;/p&gt;

&lt;p&gt;这个导数的含义是，在n维空间中f(x)所定义的(超)平面上的某个坐标点x相对于w的斜率。&lt;/p&gt;

&lt;p&gt;3) 自变量是一个矩阵(Matrix)时：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;含义和2)类似。（已经无法想象了）&lt;/p&gt;

&lt;h2&gt;矩阵迹(trace)的各种性质&lt;/h2&gt;

&lt;h3&gt;性质1&lt;/h3&gt;

&lt;p&gt;\[ tr(A) = tr(A^{T}) \]&lt;/p&gt;

&lt;h3&gt;性质2&lt;/h3&gt;

&lt;p&gt;\[ tr(AB) = tr(BA) \]&lt;/p&gt;

&lt;p&gt;\[ tr(ABC) = tr(CAB) = tr(BCA) \]&lt;/p&gt;

&lt;p&gt;\[ tr(ABCD) = tr(DABC) = tr(CDAB) = tr(BCDA) \]&lt;/p&gt;

&lt;p&gt;(看出规律了吧)&lt;/p&gt;

&lt;h3&gt;性质3&lt;/h3&gt;

&lt;p&gt;\[ tr(A+B) = tr(A) + tr(B) \]&lt;/p&gt;

&lt;h3&gt;性质4&lt;/h3&gt;

&lt;p&gt;\[ tr(\alpha A) = \alpha tr(A) \]&lt;/p&gt;

&lt;h3&gt;性质5&lt;/h3&gt;

&lt;p&gt;设有矩阵H、U，H和U都是n x m的矩阵，则有：&lt;/p&gt;

&lt;p&gt;\[ \sum _{j=1}^{m} \sum _{i=1}^{n}(h_{ij}u_{ij}) = \sum _{j=1}^{m} \sum _{i=1}^{n}((h^{T})_{ji}u_{ij}) = tr(H^{T}U) \]&lt;/p&gt;

&lt;h1&gt;矩阵微分的各种性质&lt;/h1&gt;

&lt;p&gt;设有关于矩阵A的一个函数f，记为\( f(A) \)，\( f(A) \)关于A的导数为：&lt;/p&gt;

&lt;p&gt;\[  \nabla _{A}f(A) = \frac { \partial f(A) }{ \partial A } \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{12}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{1n}}\\ \frac {\partial f }{\partial A_{21}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{2n}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{m1}}&amp;amp;\frac {\partial f }{\partial A_{m2}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;h2&gt;性质1&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ A^{T} }f(A) = (\nabla _{A}f(A))^{T} \]&lt;/p&gt;

&lt;p&gt;证明：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ A^{T} }f(A) = \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{21}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{m1}}\\ \frac {\partial f }{\partial A_{12}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{m2}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{1n}}&amp;amp;\frac {\partial f }{\partial A_{2n}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;\[  =  \left[ \begin{matrix} \frac {\partial f }{\partial A_{11}}&amp;amp;\frac {\partial f }{\partial A_{12}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{1n}}\\ \frac {\partial f }{\partial A_{21}}&amp;amp;\frac {\partial f }{\partial A_{22}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{2n}}\\ \vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\ \frac {\partial f }{\partial A_{m1}}&amp;amp;\frac {\partial f }{\partial A_{m2}}&amp;amp;\cdots &amp;amp;\frac {\partial f }{\partial A_{mn}}\\ \end{matrix} \right]^{T} \]&lt;/p&gt;

&lt;p&gt;\[  = (\frac { \partial f(A) }{ \partial A })^{T}  = (\nabla _{A}f(A))^{T} \]&lt;/p&gt;

&lt;h2&gt;性质2&lt;/h2&gt;

&lt;p&gt;假设存在矩阵U，使得下面的等式成立：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} = tr(W^{T}U) \]&lt;/p&gt;

&lt;p&gt;（这里要注意一下：中间的式子的分子包含矩阵，而分母只是一个t，那么这个极限的结果应仍然仍是一个矩阵，然而等式右边却是一个trace，trace是一个数。矩阵等于数？其实是可以的，譬如当f(X)是一个tr运算的时候。）&lt;/p&gt;

&lt;p&gt;那么，对\( \textbf {W} \)中任意一个\(W_{ij} \)求导，则有：&lt;/p&gt;

&lt;p&gt;\[ D_{W_{ij}}f(\textbf {X}) = tr(W_{ij}^{T}U) = \sum _{j=1}^{} \sum _{i=1}^{}(w_{ij}u_{ij}) = u_{ij}  \]&lt;/p&gt;

&lt;p&gt;这个结果可能有点费解。首先要明白\( W_{ij} \)是W矩阵的一个元素，是一个值(Scalar)，那么就可以确定这个导数也是一个值(Scalar)；对W矩阵的局部单个元素求导，其实按偏导数的概念理解即可，既然是偏导数，这就意味着除了存在\( w_{ij} \)的那一项之外的其他元素都被当做常数，而对常数求导必然等于0，所以最后会得到唯一的\( u_{ij}\)。(中间那一步用到了trace的性质5)&lt;/p&gt;

&lt;p&gt;既然已经知道，对局部的\( W_{ij} \)求导会得到\( U_{ij}\)，那么分别对所有\( W_{ij} \)求导，并把各个求导结果再组成一个矩阵，就是U矩阵了。又因为W代表任意矩阵，所以f(X)关于X的导数等于U：&lt;/p&gt;

&lt;p&gt;\[  \frac { \partial f(\textbf {X}) }{ \partial X } = \textbf {U} \]&lt;/p&gt;

&lt;p&gt;这个式子的意义在于，当题目是“给你一个自变量是矩阵X的函数f(X),求它关于X的导数”时，可以把问题立即转变成求U，而U的求解，可以通过上面的标准导数公式来求。小结一下步骤：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;计算\( \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \)，并化简，直到得到一个形如\(tr(W^{T}Q) \)的式子；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据\( \frac { \partial f(\textbf {X}) }{ \partial X } = \textbf {U} \)，可以知道\(tr(W^{T}Q) \) = \(tr(W^{T}U) \)， 于是就得到了\(\frac { \partial f(\textbf {X}) }{ \partial X } = U = Q \) 。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;性质3&lt;/h2&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } = A^{T} \]&lt;/p&gt;

&lt;p&gt;证明过程需要用到上面的性质2，刚好作为一个应用举例。&lt;/p&gt;

&lt;p&gt;证明，设：&lt;/p&gt;

&lt;p&gt;\[ f(X) = tr(AX)   \]&lt;/p&gt;

&lt;p&gt;根据上面的结论，只需要把下面这个极限简化，理论上就可以求出 \( \frac { \partial tr(AX) }{ \partial X } \) 了：&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(A(X + tW)) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AX + AtW) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AX) + tr(AtW) -  tr(AX) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AtW) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(AW)t }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} tr(AW) \]&lt;/p&gt;

&lt;p&gt;\[  = tr(AW) \]&lt;/p&gt;

&lt;p&gt;\[  = tr((AW)^{T}) \]&lt;/p&gt;

&lt;p&gt;\[  = tr(W^{T}A^{T}) \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ D_{W}f(X) = tr(W^{T}A^{T}) = tr(W^{T}U)  \]&lt;/p&gt;

&lt;p&gt;\[ U = A^{T} \]&lt;/p&gt;

&lt;p&gt;得证：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } = U =  A^{T}  \] &lt;/p&gt;

&lt;h2&gt;性质4&lt;/h2&gt;

&lt;p&gt;\[ \frac { \partial tr(X^{T}A^{T}) }{ \partial X } = A^{T} \]&lt;/p&gt;

&lt;p&gt;有了性质3，就可以推导出这个：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AX) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr((AX)^{T}) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(X^{T}A^{T}) }{ \partial X } =  A^{T}  \] &lt;/p&gt;

&lt;h2&gt;性质5&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(X) = tr(\nabla _{ X}X) \]&lt;/p&gt;

&lt;p&gt;待证。&lt;/p&gt;

&lt;h2&gt;性质6&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(AXBX^{T}C) = A^{T}C^{T}XB^{T} + CAXB \]&lt;/p&gt;

&lt;p&gt;类似性质3的证明过程，只是复杂一些。设:&lt;/p&gt;

&lt;p&gt;\[ f(X) = tr(AXBX^{T}C)  \]&lt;/p&gt;

&lt;p&gt;\[ D_{\textbf {W}}f(\textbf {X}) = \lim _{t\to 0} \frac {f(\textbf {X}+t\textbf {W})-f(\textbf {X})}{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr(A(X + tW)B(X + tW)^{T}C) -  tr(AXBX^{T}C) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( A(X + tW)B(X + tW)^{T}C - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( (AXB + tAWB)(X^{T}C + tW^{T}C) - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( AXBX^{T}C + tAWBX^{T}C + tAXBW^{T}C + t^{2}AWBW^{T}C - AXBX^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( tAWBX^{T}C + tAXBW^{T}C + t^{2}AWBW^{T}C ) }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} \frac {  tr( AWBX^{T}C + AXBW^{T}C + tAWBW^{T}C )t }{t} \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C + tAWBW^{T}C )] \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C)] + \lim _{t\to 0} [tr( tAWBW^{T}C )] \]&lt;/p&gt;

&lt;p&gt;\[  = \lim _{t\to 0} [tr( AWBX^{T}C + AXBW^{T}C)] \]&lt;/p&gt;

&lt;p&gt;\[  = tr( AWBX^{T}C ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( (AWBX^{T}C)^{T} ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( C^{T}XB^{T}W^{T}A^{T} ) + tr( AXBW^{T}C ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}A^{T}C^{T}XB^{T} ) + tr( W^{T}CAXB ) 【用了trace的性质2】 \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}A^{T}C^{T}XB^{T} + W^{T}CAXB ) \]&lt;/p&gt;

&lt;p&gt;\[  = tr( W^{T}  (A^{T}C^{T}XB^{T} + CAXB) ) \]&lt;/p&gt;

&lt;p&gt;所以有：&lt;/p&gt;

&lt;p&gt;\[ D_{W}f(X) = tr( W^{T}  (A^{T}C^{T}XB^{T} + CAXB) ) = tr(W^{T}U)  \]&lt;/p&gt;

&lt;p&gt;得证：&lt;/p&gt;

&lt;p&gt;\[ \frac { \partial tr(AXBX^{T}C) }{ \partial X } = U = A^{T}C^{T}XB^{T} + CAXB \]&lt;/p&gt;

&lt;p&gt;在wiki&lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus&quot;&gt;Matrix Calculus&lt;/a&gt;还给出了用性质5公式：\( \nabla _{ X}tr(X) = tr(\nabla _{ X}X) \)推导性质6。为了方便，把图也贴进来吧：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/c/6/1/c61612bff41e8572a97d977871ce2be2.png&quot; alt=&quot;https://upload.wikimedia.org/math/c/6/1/c61612bff41e8572a97d977871ce2be2.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/math/1/e/7/1e761891d19891ff75670424341b8425.png&quot; alt=&quot;https://upload.wikimedia.org/math/1/e/7/1e761891d19891ff75670424341b8425.png&quot;&gt;&lt;/p&gt;

&lt;p&gt;它最后得到的式子和我的推导似乎不一样，但其实是一样的，用trace的性质1\( tr(A) = tr(A^{T}) \)可以转换得到。&lt;/p&gt;

&lt;h2&gt;性质7&lt;/h2&gt;

&lt;p&gt;\[ \nabla _{ X}tr(XBX^{T}C) = C^{T}XB^{T} + CXB \]&lt;/p&gt;

&lt;p&gt;证明：把性质6的A设为单位矩阵E，得到：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ X}tr(EXBX^{T}C) = E^{T}C^{T}XB^{T} + CEXB \]&lt;/p&gt;

&lt;p&gt;化简得到：&lt;/p&gt;

&lt;p&gt;\[ \nabla _{ X}tr(XBX^{T}C) = C^{T}XB^{T} + CXB \]&lt;/p&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.tc.umn.edu/%7Enydic001/docs/unpubs/Schonemann_Trace_Derivatives_Presentation.pdf&quot;&gt;http://www.tc.umn.edu/~nydic001/docs/unpubs/Schonemann_Trace_Derivatives_Presentation.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 08 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/matrix-calculus-1/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/matrix-calculus-1/</guid>
      </item>
    
      <item>
        <title>最小二乘估计(Least Squares Estimator)的公式的推导</title>
        <description>&lt;p&gt;最近在学习ML(Machine Learning)，注意到了一个有趣的东西：&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)&quot;&gt;Least Squares Estimator&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;先从简单说起吧。看下面的式子：&lt;/p&gt;

&lt;p&gt;\[ y = ax + e \]&lt;/p&gt;

&lt;p&gt;这是一个非常简单的直线方程。如果赋予y、a、x、b具体的意义，这个式子就有意思了：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;假设x是一个统计变量（预先就知道的），譬如，x代表人的年龄。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;假设y是关于x的一个label量（预先就知道的），譬如，y代表的是年龄为x时的人的智商。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;假设y和x存在线性关系，那么可以有 y = ax。这个式子表明年龄为x时，智商为ax。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;当x、y的取值只有一对时，a = y/x，但当x、y不只一对时，y = ax可能会无解（因为求解的是方程组 \( y_{i} = ax_{i} \) 了）&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了使方程组 \( y_{i} = ax_{i} \) 可以求解，需要把方程组扩展成 \( y_{i} = ax_{i} + e_{i} \) 。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;\( y_{i} = ax_{i} + e_{i} \)使得我们有机会求出a，但同时也产生了很多个\( e_{i} \)。每对&lt;y,  x&gt;都有它自己的error系数的话，这个a的意义就减弱了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了使得a变得更有意义，我们希望每个error系数尽可能地小（无限逼近0最好了），同时又能求出唯一的a。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;又因为现实生活中，智商肯定不只跟年龄x有关系，还和其他参数有关系，那么可以再把公式扩展成:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;\[ y_{i} = a_{1}x_{i1} + a_{2}x_{i2} + \cdots + a_{k}x_{ik} + e_{i} ,   1\le i\le n,   k\ge 1 \]&lt;/p&gt;

&lt;p&gt;现在，把上式写成矩阵形式：&lt;/p&gt;

&lt;p&gt;\[ \vec y = X\vec a + \vec e \]&lt;/p&gt;

&lt;p&gt;\[  \left[ \begin{matrix} y_{1}\\   y_{2}\\   \vdots \\   y_{n}\\ \end{matrix} \right] =  \left[ \begin{matrix} x_{11}&amp;amp;  x_{12}&amp;amp;  \cdots &amp;amp;  x_{1k}\\      x_{21}&amp;amp;  x_{22}&amp;amp;  \cdots &amp;amp;  x_{2k}\\      \vdots &amp;amp;  \vdots &amp;amp;  \ddots &amp;amp;  \vdots \\     x_{n1}&amp;amp;  x_{n2}&amp;amp;  \cdots &amp;amp;  x_{nk}\\ \end{matrix} \right] \left[ \begin{matrix} a_{1}\\   a_{2}\\   \vdots \\   a_{k}\\ \end{matrix} \right] +  \left[ \begin{matrix} e_{1}\\   e_{2}\\   \vdots \\   e_{n}\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;再回到上面的第7点：为了使得\(\vec a\)变得更有意义，我们希望\(\vec e\)的每个分量尽可能地小。明确这一点非常重要。&lt;/p&gt;

&lt;p&gt;那么，这个目标完成情况应该如何衡量？其实很简单，既然\(\vec e\)是一个向量（n维空间），那么\(\vec e\)的长度就是我们需要的指标：&lt;/p&gt;

&lt;p&gt;\[ |\vec e| = \sqrt { \sum ^{n}_{i=1}e_{i}^{2} } \]&lt;/p&gt;

&lt;p&gt;开根号是不必要的，我们可以换成下面这个指标：&lt;/p&gt;

&lt;p&gt;\[ |\vec e|^{2} = \sum ^{n}_{i=1}e_{i}^{2} = \vec e\vec e = \vec e^{T}\vec e \]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;小结一下：当\( \vec e^{T}\vec e \)取得最小值时，\(\vec a\)能取得最优解。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;继续推导。&lt;/p&gt;

&lt;p&gt;由上文可知：&lt;/p&gt;

&lt;p&gt;\( \vec e = \vec y - X\vec a \)&lt;/p&gt;

&lt;p&gt;\( \vec e^{T} = (\vec y - X\vec a)^{T} \)&lt;/p&gt;

&lt;p&gt;\( \vec e^{T}\vec e = (\vec y - X\vec a)^{T}(\vec y - X\vec a)  \)&lt;/p&gt;

&lt;p&gt;\( = (\vec y^{T} - \vec a^{T}X^{T})(\vec y - X\vec a)  \)&lt;/p&gt;

&lt;p&gt;\( = \vec y^{T}\vec y - \vec a^{T}X^{T}\vec y - \vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \)&lt;/p&gt;

&lt;p&gt;注意，中间的2个子项是可以合并的。首先，仔细观察\( \vec a^{T}X^{T}\vec y \)这个子项，发现它是一个&lt;strong&gt;值&lt;/strong&gt;，那么就有：&lt;/p&gt;

&lt;p&gt;\( \vec a^{T}X^{T}\vec y  = (\vec a^{T}X^{T}\vec y)^{T} \)&lt;/p&gt;

&lt;p&gt;（一个数值可认为是一个1维的方阵，1维方阵的转置矩阵是它本身）&lt;/p&gt;

&lt;p&gt;而又有：&lt;/p&gt;

&lt;p&gt;\( (\vec a^{T}X^{T}\vec y)^{T} = \vec y^{T}(\vec a^{T}X^{T})^{T} \)&lt;/p&gt;

&lt;p&gt;\( = \vec y^{T}(X\vec a) = \vec y^{T}X\vec a  \)&lt;/p&gt;

&lt;p&gt;得：&lt;/p&gt;

&lt;p&gt;\( \vec a^{T}X^{T}\vec y  = \vec y^{T}X\vec a  \)&lt;/p&gt;

&lt;p&gt;所以上面的方程可变为：&lt;/p&gt;

&lt;p&gt;\[ \vec e^{T}\vec e = \vec y^{T}\vec y - 2\vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \]&lt;/p&gt;

&lt;p&gt;如何让\( \vec e^{T}\vec e \)取得最小值？此时需要使用新的招数：&lt;strong&gt;矩阵微分&lt;/strong&gt;。&lt;/p&gt;

&lt;h2&gt;矩阵微分&lt;/h2&gt;

&lt;p&gt;矩阵微分公式：&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \vec y = A\vec x \]&lt;/p&gt;

&lt;p&gt;y是一个\(m \times 1\)的矩阵，A是一个\(m \times n\)的矩阵，x是一个\(n \times 1\)的矩阵。&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \vec y}{\partial \vec x} = A 【公式1】 \]&lt;/p&gt;

&lt;p&gt;这是如何得到的呢？实际上超级简单，上面这个式子指的是，\(\vec y \)的每一个分量对\(\vec x \)的每一个分量的微分，结果显然就是一个\(m \times n\)矩阵。&lt;/p&gt;

&lt;p&gt;扩展公式：&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \alpha = \vec y^{T}A\vec x \]&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec x} = \vec y^{T}A  【公式2】 \]&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec y} = \vec x^{T}A^{T}  【公式3】 \]&lt;/p&gt;

&lt;p&gt;设：&lt;/p&gt;

&lt;p&gt;\[ \alpha = \vec x^{T}A\vec x \]&lt;/p&gt;

&lt;p&gt;且A是对称矩阵，&lt;/p&gt;

&lt;p&gt;则有：&lt;/p&gt;

&lt;p&gt;\[ \frac {\partial \alpha }{\partial \vec x} = 2\vec x^{T}A  【公式4】 \]&lt;/p&gt;

&lt;h2&gt;应用矩阵微分公式&lt;/h2&gt;

&lt;p&gt;再来看下刚才的\( \vec e^{T}\vec e  \)方程：&lt;/p&gt;

&lt;p&gt;\[ \vec e^{T}\vec e  = \vec y^{T}\vec y - 2\vec y^{T}X\vec a + \vec a^{T}X^{T}X\vec a \]&lt;/p&gt;

&lt;p&gt;对等号右边的式子求关于\(\vec a\)的微分，得到：&lt;/p&gt;

&lt;p&gt;\( \frac {\partial \vec y^{T}\vec y}{\partial \vec a} - 2\frac {\partial \vec y^{T}X\vec a}{\partial \vec a} + \frac {\partial \vec a^{T}X^{T}X\vec a}{\partial \vec a} \)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;当这个式子(导数)等于0时,   就得到了\( \vec e^{T}\vec e \)的最小值。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;显然，第一个子项为0，所以可把它去掉，得到：&lt;/p&gt;

&lt;p&gt;\( - 2\frac {\partial \vec y^{T}X\vec a}{\partial \vec a} + \frac {\partial \vec a^{T}X^{T}X\vec a}{\partial \vec a}  = 0\)&lt;/p&gt;

&lt;p&gt;\(  2\frac {\vec y^{T}X\vec a}{\partial \vec a} = \frac {\vec a^{T}X^{T}X\vec a}{\partial \vec a} \)&lt;/p&gt;

&lt;p&gt;观察左边的式子，和上文的【公式2】是一样的，所以有：&lt;/p&gt;

&lt;p&gt;\( 2\frac {\vec y^{T}X\vec a}{\partial \vec a} = 2\vec y^{T}X \)&lt;/p&gt;

&lt;p&gt;观察右边的式子，符合上文的【公式4】，所以有：&lt;/p&gt;

&lt;p&gt;\(  \frac {\vec a^{T}X^{T}X\vec a}{\partial \vec a} = 2\vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;综上，得：&lt;/p&gt;

&lt;p&gt;\(  2\vec y^{T}X = 2\vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;\(  \vec y^{T}X = \vec a^{T}X^{T}X \)&lt;/p&gt;

&lt;p&gt;\(  (\vec y^{T}X)^{T} = (\vec a^{T}X^{T}X)^{T} \)&lt;/p&gt;

&lt;p&gt;\(  X^{T}\vec y = X^{T}X\vec a \)&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;这个东西就是所谓的&lt;strong&gt;最小二乘估计(Least Squares Estimator)&lt;/strong&gt;了。&lt;/p&gt;

&lt;h2&gt;特殊情况下的最小二乘估计&lt;/h2&gt;

&lt;p&gt;上文的讨论中没有考虑到一种特殊情况：X是一个方阵。当X是方阵时，上面的公式可进一步简化：&lt;/p&gt;

&lt;p&gt;\[ \vec a = (X^{T}X)^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}(X^{T})^{-1}X^{T}\vec y  \]&lt;/p&gt;

&lt;p&gt;\[ \vec a = X^{-1}\vec y  \]&lt;/p&gt;

&lt;p&gt;还是搞不明白X不是方阵时，公式为什么要那么复杂？答案就在于，X不是方阵时，\( X^{-1} \)和\( (X^{T})^{-1} \)都不成立，导致最小二乘估计公式不能简化。&lt;/p&gt;

&lt;p&gt;关于这个问题的进一步讨论，请阅读我的另一篇文章&lt;a href=&quot;http://www.qiujiawei.com/linear-algebra-16/&quot;&gt;线性代数之伪逆矩阵&lt;/a&gt;。&lt;/p&gt;

&lt;h2&gt;实例&lt;/h2&gt;

&lt;p&gt;使用ML中常用的Iris数据集&lt;a href=&quot;http://archive.ics.uci.edu/ml/datasets/Iris&quot;&gt;http://archive.ics.uci.edu/ml/datasets/Iris&lt;/a&gt;来验证下上面的公式是否可用。&lt;/p&gt;

&lt;p&gt;Iris鸢尾花卉数据集，是一类多重变量分析的数据集。通过&lt;strong&gt;花萼长度，花萼宽度，花瓣长度，花瓣宽度4个属性&lt;/strong&gt;预测鸢尾花卉属于（Setosa，Versicolour，Virginica）&lt;strong&gt;三个种类&lt;/strong&gt;中的哪一类。&lt;/p&gt;

&lt;p&gt;整个数据集可在&lt;a href=&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/bezdekIris.data&quot;&gt;这里&lt;/a&gt;浏览。&lt;/p&gt;

&lt;p&gt;首先是为3个类别各赋予1个标签值：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Iris-setosa = 1&lt;/li&gt;
&lt;li&gt;Iris-versicolor = 2&lt;/li&gt;
&lt;li&gt;Iris-virginica = 3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后从整个数据集中挑选训练数据集(Train Dataset)，譬如从3个类别中各取出前10个数据项。&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;6.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;7.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此时，已经得到了\( X \)、\( \vec y \)的值了，\( X \)是上面这个表格的前4列（30x4的矩阵），\( \vec y \)是第5列（30x1的矩阵）。&lt;/p&gt;

&lt;p&gt;我们的目标是求出系数\( \vec a \)，它是一个4x1的矩阵。&lt;/p&gt;

&lt;p&gt;根据前文推导出来的公式：&lt;/p&gt;

&lt;p&gt;\(  \vec a = (X^{T}X)^{-1}X^{T}\vec y  \)&lt;/p&gt;

&lt;p&gt;因为矩阵比较庞大的关系，只能直接给出\( (X^{T}X)^{-1}X^{T} \)的结果了，读者们也可以自己做下计算：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;7.2&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.5&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.7&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.6&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;3.9&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.6&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;4.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;5.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;6.1&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.4&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.9&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.2&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.1&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.7&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;1.8&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;1051.290&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;532.720&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;723.350&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;230.480&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;532.720&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;281.280&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;345.450&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;108.190&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;723.350&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;345.450&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;550.410&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;182.580&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;230.480&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;108.190&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;182.580&lt;/span&gt;     &lt;span class=&quot;mf&quot;&gt;62.220&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;0.541034&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.573805&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.641245&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.875298&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.573805&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.633743&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.631976&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.830927&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.641245&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.631976&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.920228&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.42389&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.875298&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.830927&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.42389&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;2.396868&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;0.028273&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.206968&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.048125&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.076847&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.083211&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.056253&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.097334&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.032575&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.006168&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.002067&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.162628&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.053786&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.125186&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.228843&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.273287&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.270475&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.098417&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.033124&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.094950&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.011335&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.144267&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.018560&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.174707&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.270956&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.001741&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.003648&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.181042&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.178793&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.046731&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.106397&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.010276&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.191835&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.013523&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.106879&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.131031&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.115039&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.150711&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.067480&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.031694&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.017830&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.181668&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.046873&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.144359&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.250620&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.294553&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.267479&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.117184&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.036068&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.122374&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.028729&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.190919&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.027353&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.189075&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.26628&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.008918&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.033594&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.204029&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.134858&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.090344&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.072185&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.054892&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.242631&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.080010&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.104963&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.072430&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.003185&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.060144&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.038057&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.014794&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.054978&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.134766&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.076454&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.092183&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.243448&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.301346&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.404405&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.092525&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.012496&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.017495&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.032696&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.007320&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.025114&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.217735&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.383162&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.067400&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.105802&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.158253&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.386076&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.057919&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.288185&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;0.041703&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.282107&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.083251&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.205964&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.12892&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.024128&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.073167&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.105123&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.072449&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.183062&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.131452&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.130738&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.081924&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.323375&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.408249&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.628974&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.084976&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.007234&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.066687&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.110491&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.221148&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.125436&lt;/span&gt;    &lt;span class=&quot;mf&quot;&gt;0.354307&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.554733&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.211204&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.204767&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.121187&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.676157&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.157020&lt;/span&gt;   &lt;span class=&quot;mf&quot;&gt;0.617249&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.291&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.441&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.637&lt;/span&gt;   
&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.094&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;a的值为：&lt;/p&gt;

&lt;p&gt;\[ \vec a =  \left[ \begin{matrix} -0.291\\ 0.441\\ 0.637\\ -0.094\\ \end{matrix} \right] \]&lt;/p&gt;

&lt;p&gt;然后就是测试a的可靠度如何。方法就是把a用到剩余的其他数据项里，算出predict值，并和实际的值做比较，看预测正确的有多少个，公式为：&lt;/p&gt;

&lt;p&gt;\[ \vec y_{predict} = X_{test}\vec a \]&lt;/p&gt;

&lt;p&gt;结果是：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mf&quot;&gt;0.997&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.103&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.809&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.763&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.822&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.200&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.939&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.923&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.072&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.119&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.992&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.066&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.867&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.007&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.294&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.868&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.026&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.967&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.859&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.044&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.971&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.846&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.241&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.125&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.887&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.702&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.752&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.887&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.852&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.952&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.888&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.505&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.940&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.051&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.364&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.790&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.192&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.946&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.026&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;0.873&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.563&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.140&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.678&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.366&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.820&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.089&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.419&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.021&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.892&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.854&lt;/span&gt; 
&lt;span class=&quot;mf&quot;&gt;2.583&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.886&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.250&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.341&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.033&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.074&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.182&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.398&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.258&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.623&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.775&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.721&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.874&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.543&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.477&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.470&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.270&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.862&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.183&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.928&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.236&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.346&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.894&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.567&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.114&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.227&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.173&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.092&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;1.426&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.066&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.580&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.526&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.650&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.441&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.570&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.709&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.766&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.496&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.085&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.268&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.818&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.539&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.074&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.310&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.939&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.969&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.319&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.500&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.742&lt;/span&gt;  
&lt;span class=&quot;mf&quot;&gt;2.772&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.788&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.266&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.733&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.509&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.807&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.752&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;3.008&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.839&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.465&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.602&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.759&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.392&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.573&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.975&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.902&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.470&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.276&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.556&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.919&lt;/span&gt;   
&lt;span class=&quot;mf&quot;&gt;2.686&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;再四舍五入一下，得到：&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c&quot; data-lang=&quot;c&quot;&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;此时就可以算准确率了，经过比较，上面的predict值正确的有109个，总共的测试项有120，准确率高达90.83%哦。&lt;/p&gt;

&lt;h2&gt;参考资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://economictheoryblog.com/2015/02/19/ols_estimator/&quot;&gt;https://economictheoryblog.com/2015/02/19/ols_estimator/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.atmos.washington.edu/%7Edennis/MatrixCalculus.pdf&quot;&gt;http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 06 May 2016 00:00:00 +0800</pubDate>
        <link>http://www.qiujiawei.com/linear-algebra-15/</link>
        <guid isPermaLink="true">http://www.qiujiawei.com/linear-algebra-15/</guid>
      </item>
    
  </channel>
</rss>